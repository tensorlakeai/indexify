use std::{collections::HashMap, error::Error, path::PathBuf, sync::Arc, time::Duration};

use serde::Deserialize;
use tokio::{
    sync::{Mutex, Notify},
    time::{sleep, timeout},
};
use tonic::{
    transport::{Certificate, Channel, ClientTlsConfig, Endpoint, Identity},
    Request,
};
use tracing::{error, info, warn};

use crate::executor::{
    blob_store::BlobStore,
    executor_api::executor_api_pb::{
        executor_api_client::ExecutorApiClient, AllocationResult, AllowedFunction,
        DesiredExecutorState, ExecutorState, ExecutorStatus, ExecutorUpdate, FunctionCallWatch,
        FunctionExecutorState, ReportExecutorStateRequest,
    },
    function_executor::server_factory::SubprocessFunctionExecutorServerFactory,
    function_executor_controller::FunctionExecutorController,
    host_resources::{HostResources, HostResourcesProvider},
    monitoring::health_checker::{generic_health_checker::GenericHealthChecker, HealthChecker},
};

const REPORTING_INTERVAL_SEC: u64 = 5;
const REPORTING_BACKOFF_SEC: u64 = 5;
const REPORT_RPC_TIMEOUT_SEC: u64 = 5;

#[allow(non_camel_case_types)] // The autogenerated code uses snake_case types in some cases
pub mod executor_api_pb {
    tonic::include_proto!("executor_api_pb");
}

pub const CONNECTION_TIMEOUT: Duration = Duration::from_secs(5);

#[derive(Debug, Clone)]
pub struct FunctionCallWatchInfo {
    watch: FunctionCallWatch,
    ref_counter: u64,
}

fn function_call_watch_key(watch: FunctionCallWatch) -> String {
    format!(
        "{}.{}.{}.{}",
        watch.namespace(),
        watch.application(),
        watch.request_id(),
        watch.function_call_id()
    )
}

#[derive(Debug)]
struct ReporterState {
    executor_status: ExecutorStatus,
    last_server_clock: u64,
    pending_allocation_results: Vec<AllocationResult>,
    function_executor_states: HashMap<String, FunctionExecutorState>,
    function_call_watches: HashMap<String, FunctionCallWatchInfo>,
    last_state_report_request: Option<ReportExecutorStateRequest>,
}

impl ReporterState {
    fn new() -> Self {
        Self {
            executor_status: ExecutorStatus::Unknown,
            last_server_clock: 0,
            pending_allocation_results: Vec::new(),
            function_executor_states: HashMap::new(),
            function_call_watches: HashMap::new(),
            last_state_report_request: None,
        }
    }

    fn current_function_call_watches(&self) -> Vec<FunctionCallWatch> {
        self.function_call_watches
            .values()
            .map(|info| info.watch.clone())
            .collect()
    }

    fn remove_pending_update(&mut self, executor_id: &str) -> ExecutorUpdate {
        let alloc_results = std::mem::take(&mut self.pending_allocation_results);
        ExecutorUpdate {
            executor_id: Some(executor_id.to_string()),
            allocation_results: alloc_results,
        }
    }

    fn add_to_pending_update(&mut self, update: ExecutorUpdate) {
        self.pending_allocation_results
            .extend(update.allocation_results);
    }
}

#[derive(Debug, Clone)]
pub struct ExecutorStateReporter {
    id: String,
    version: String,
    labels: HashMap<String, String>,
    channel_manager: Arc<ChannelManager>,
    total_host_resources: HostResources,
    total_function_executor_resources: HostResources,
    health_checker: Arc<Mutex<GenericHealthChecker>>,
    catalog_entry_name: Option<String>,
    allowed_functions: Vec<AllowedFunction>,
    state: Arc<Mutex<ReporterState>>,
    state_report_scheduled: Arc<Notify>,
    state_reported: Arc<Notify>,
}

impl ExecutorStateReporter {
    pub fn new(
        id: String,
        version: String,
        labels: &mut HashMap<String, String>,
        function_allowlist: Vec<FunctionUri>,
        channel_manager: Arc<ChannelManager>,
        host_resource_provider: HostResourcesProvider,
        health_checker: Arc<Mutex<GenericHealthChecker>>,
        catalog_entry_name: Option<String>,
    ) -> Self {
        labels.extend(executor_labels());

        ExecutorStateReporter {
            id,
            version,
            labels: labels.to_owned(),
            channel_manager,
            total_host_resources: host_resource_provider.total_host_resources(),
            total_function_executor_resources: host_resource_provider
                .total_function_executor_resources(),
            health_checker,
            catalog_entry_name,
            allowed_functions: to_allowed_function(function_allowlist),
            state: Arc::new(Mutex::new(ReporterState::new())),
            state_report_scheduled: Arc::new(Notify::new()),
            state_reported: Arc::new(Notify::new()),
        }
    }

    pub async fn last_state_report_request(&self) -> Option<ReportExecutorStateRequest> {
        self.state.lock().await.last_state_report_request.clone()
    }

    pub async fn update_executor_status(&mut self, status: ExecutorStatus) {
        self.state.lock().await.executor_status = status;
    }

    pub async fn update_last_server_clock(&mut self, clock: u64) {
        self.state.lock().await.last_server_clock = clock;
    }

    pub async fn update_function_executor_state(&mut self, state: FunctionExecutorState) {
        match state.clone().description {
            Some(desc) => match desc.id {
                Some(id) => {
                    self.state
                        .lock()
                        .await
                        .function_executor_states
                        .insert(id.to_string(), state);
                }
                None => {}
            },
            None => {}
        }
    }

    pub async fn remove_function_executor_state(&mut self, id: &str) {
        if self
            .state
            .lock()
            .await
            .function_executor_states
            .remove(id)
            .is_none()
        {
            warn!(
                function_executor_id = id,
                "attempted to remove non-existing function executor state"
            )
        }
    }

    pub async fn add_completed_allocation_result(&mut self, alloc_result: AllocationResult) {
        self.state
            .lock()
            .await
            .pending_allocation_results
            .push(alloc_result);
    }

    pub async fn add_function_call_watcher(&mut self, watch: FunctionCallWatch) {
        let mut state = self.state.lock().await;
        let content_derived_key = function_call_watch_key(watch.clone());
        if !state
            .function_call_watches
            .contains_key(&content_derived_key)
        {
            state.function_call_watches.insert(
                content_derived_key.clone(),
                FunctionCallWatchInfo {
                    watch,
                    ref_counter: 0,
                },
            );
        }
        match state.function_call_watches.get_mut(&content_derived_key) {
            Some(info) => info.ref_counter += 1,
            None => {}
        }
    }

    pub async fn remove_function_call_watcher(&mut self, watch: FunctionCallWatch) {
        let content_derived_key = function_call_watch_key(watch.clone());
        match self
            .state
            .lock()
            .await
            .function_call_watches
            .get_mut(&content_derived_key)
        {
            Some(info) => {
                if info.ref_counter == 0 {
                    self.state
                        .lock()
                        .await
                        .function_call_watches
                        .remove(&content_derived_key);
                    return;
                }
                info.ref_counter -= 1;
            }
            None => {}
        }
    }

    pub async fn current_function_call_watches(&self) -> Vec<FunctionCallWatch> {
        return self
            .state
            .lock()
            .await
            .function_call_watches
            .iter()
            .map(|f| f.1.watch.clone())
            .collect();
    }

    pub fn schedule_state_report(&self) {
        self.state_report_scheduled.notify_one();
    }

    pub async fn report_state_and_wait_for_completion(&self) {
        let notified = self.state_reported.notified();
        self.schedule_state_report();
        notified.await;
    }

    pub async fn run(&self) {
        let notify = self.state_report_scheduled.clone();
        tokio::spawn(async move {
            Self::periodic_scheduler_loop(notify).await;
        });

        let worker_self = self.clone();
        tokio::spawn(async move {
            worker_self.state_report_worker_loop().await;
        });

        info!("executor state reporter started");
    }

    async fn current_executor_state(&self) -> ExecutorState {
        let state = self.state.lock().await;
        let function_executor_states: Vec<FunctionExecutorState> =
            state.function_executor_states.values().cloned().collect();

        ExecutorState {
            executor_id: Some(self.id.clone()),
            hostname: Some(gethostname::gethostname().to_string_lossy().to_string()),
            version: Some(self.version.clone()),
            status: Some(state.executor_status as i32),
            total_function_executor_resources: Some(
                self.total_function_executor_resources.to_proto(),
            ),
            total_resources: Some(self.total_host_resources.to_proto()),
            allowed_functions: self.allowed_functions.clone(),
            function_executor_states,
            labels: self.labels.clone(),
            catalog_entry_name: self.catalog_entry_name.clone(),
            function_call_watches: state.current_function_call_watches(),
            server_clock: Some(state.last_server_clock),
            state_hash: None, // TODO: implement state hashing
        }
    }

    async fn periodic_scheduler_loop(notify: Arc<Notify>) {
        loop {
            notify.notify_one();
            sleep(Duration::from_secs(REPORTING_INTERVAL_SEC)).await;
        }
    }

    async fn state_report_worker_loop(self) {
        loop {
            let channel = match self.channel_manager.create_channel() {
                Ok(ch) => ch,
                Err(e) => {
                    error!(error = ?e, "failed to create gRPC channel");
                    sleep(Duration::from_secs(REPORTING_BACKOFF_SEC)).await;
                    continue;
                }
            };

            let mut client = ExecutorApiClient::new(channel);

            loop {
                self.state_report_scheduled.notified().await;

                let executor_state = self.current_executor_state().await;
                let executor_update = {
                    let mut state = self.state.lock().await;
                    state.remove_pending_update(&self.id)
                };

                let request = Request::new(ReportExecutorStateRequest {
                    executor_state: Some(executor_state),
                    executor_update: Some(executor_update.clone()),
                });

                self.log_reported_update(&executor_update);

                {
                    let mut state = self.state.lock().await;
                    state.last_state_report_request = Some(request.get_ref().clone());
                }

                match timeout(
                    Duration::from_secs(REPORT_RPC_TIMEOUT_SEC),
                    client.report_executor_state(request),
                )
                .await
                {
                    Ok(Ok(_response)) => {
                        self.state_reported.notify_waiters();
                        self.health_checker
                            .lock()
                            .await
                            .server_connection_state_changed(
                                true,
                                "grpc server channel is healthy".to_string(),
                            );
                    }
                    Ok(Err(status)) => {
                        error!(
                            status = ?status,
                            "failed to report state, backing off for {} sec",
                            REPORTING_BACKOFF_SEC
                        );
                        self.handle_report_failure(executor_update).await;
                        break;
                    }
                    Err(_) => {
                        error!(
                            "state report timed out, backing off for {} sec",
                            REPORTING_BACKOFF_SEC
                        );
                        self.handle_report_failure(executor_update).await;
                        break;
                    }
                }
            }
        }
    }

    async fn handle_report_failure(&self, executor_update: ExecutorUpdate) {
        {
            let mut state = self.state.lock().await;
            state.add_to_pending_update(executor_update);
        }

        self.health_checker
            .lock()
            .await
            .server_connection_state_changed(false, "grpc server channel is unhealthy".to_string());

        sleep(Duration::from_secs(REPORTING_BACKOFF_SEC)).await;
    }

    fn log_reported_update(&self, update: &ExecutorUpdate) {
        for alloc_result in &update.allocation_results {
            info!(
                allocation_id = ?alloc_result.allocation_id,
                outcome_code = alloc_result.outcome_code,
                "reporting allocation outcome"
            );
        }
    }
}

pub struct ExecutorStateReconciler {
    executor_id: String,
    function_executor_server_factory: SubprocessFunctionExecutorServerFactory,
    cache_path: PathBuf,
    blob_store: BlobStore,
    channel_manager: Arc<ChannelManager>,
    state_reporter: ExecutorStateReporter,
    // TODO: function call watch dispatcher
    function_executor_controller: HashMap<String, FunctionExecutorController>,
    shutting_down_fe_ids: Vec<String>,
    last_server_clock: Option<u64>,
    function_call_watch_info: HashMap<String, FunctionCallWatchInfo>,
    last_desired_state: Option<DesiredExecutorState>,
}

impl ExecutorStateReconciler {
    pub fn new(
        executor_id: String,
        function_executor_server_factory: SubprocessFunctionExecutorServerFactory,
        cache_path: PathBuf,
        blob_store: BlobStore,
        channel_manager: Arc<ChannelManager>,
        state_reporter: ExecutorStateReporter,
    ) -> Self {
        ExecutorStateReconciler {
            executor_id,
            function_executor_server_factory,
            cache_path,
            blob_store,
            channel_manager,
            state_reporter,
            function_executor_controller: HashMap::new(),
            shutting_down_fe_ids: Vec::new(),
            last_server_clock: None,
            function_call_watch_info: HashMap::new(),
            last_desired_state: None,
        }
    }

    pub fn get_desired_state(&self) -> Option<DesiredExecutorState> {
        self.last_desired_state.clone()
    }

    pub async fn run(&self) {}
}

fn executor_labels() -> HashMap<String, String> {
    let mut labels = HashMap::new();
    labels.insert("os".to_string(), std::env::consts::OS.to_string());
    labels.insert(
        "architecture".to_string(),
        std::env::consts::ARCH.to_string(),
    );
    labels.insert(
        "rust_major_version".to_string(),
        env!("CARGO_PKG_VERSION_MAJOR").to_string(),
    );
    labels.insert(
        "rust_minor_version".to_string(),
        env!("CARGO_PKG_VERSION_MINOR").to_string(),
    );
    labels
}

pub struct FunctionUri {
    namespace: String,
    application: String,
    compute_fn: String,
    version: Option<String>,
}

impl FunctionUri {
    pub fn new(
        namespace: String,
        application: String,
        compute_fn: String,
        version: Option<String>,
    ) -> Self {
        FunctionUri {
            namespace,
            application,
            compute_fn,
            version,
        }
    }
}

impl From<&FunctionUri> for HashMap<String, String> {
    fn from(uri: &FunctionUri) -> Self {
        let mut map = HashMap::new();
        map.insert("namespace".to_string(), uri.namespace.clone());
        map.insert("application".to_string(), uri.application.clone());
        map.insert("compute_fn".to_string(), uri.compute_fn.clone());
        if let Some(version) = &uri.version {
            map.insert("version".to_string(), version.clone());
        }
        map
    }
}

fn to_allowed_function(uris: Vec<FunctionUri>) -> Vec<AllowedFunction> {
    uris.into_iter()
        .map(|uri| AllowedFunction {
            namespace: Some(uri.namespace.clone()),
            application_name: Some(uri.application.clone()),
            function_name: Some(uri.compute_fn.clone()),
            application_version: uri.version.clone(),
        })
        .collect()
}

#[derive(Debug, Deserialize)]
struct TlsFileConfig {
    cert_path: Option<String>,
    key_path: Option<String>,
    ca_bundle_path: Option<String>,
}

#[derive(Debug, Deserialize)]
struct TlsConfig {
    #[serde(default)]
    use_tls: bool,
    tls_config: Option<TlsFileConfig>,
}

#[derive(Debug, Clone)]
pub struct ChannelManager {
    server_address: String,
    tls_config: Option<ClientTlsConfig>,
    shared_channel: Option<Arc<Mutex<Channel>>>,
}

impl ChannelManager {
    pub async fn new(
        server_address: String,
        config_path: Option<String>,
    ) -> Result<Self, Box<dyn Error>> {
        let tls_config = match config_path {
            Some(path) => Self::load_tls_config(&path).await?,
            None => None,
        };
        Ok(ChannelManager {
            server_address,
            tls_config,
            shared_channel: None,
        })
    }

    pub async fn load_tls_config(path: &str) -> Result<Option<ClientTlsConfig>, Box<dyn Error>> {
        let config: TlsConfig = serde_yaml::from_str(&tokio::fs::read_to_string(path).await?)?;
        if !config.use_tls {
            return Ok(None);
        }

        let tls = config.tls_config.ok_or("TLS config not found")?;
        let mut tls_config = ClientTlsConfig::new();

        if let (Some(cert_path), Some(key_path)) = (tls.cert_path, tls.key_path) {
            let cert = tokio::fs::read(cert_path).await?;
            let key = tokio::fs::read(key_path).await?;
            tls_config = tls_config.identity(Identity::from_pem(cert, key));
        }
        if let Some(ca_bundle_path) = tls.ca_bundle_path {
            let ca_bundle = tokio::fs::read(ca_bundle_path).await?;
            tls_config = tls_config.ca_certificate(Certificate::from_pem(ca_bundle));
        }

        Ok(Some(tls_config))
    }

    pub fn get_shared_channel(&mut self) -> Result<Arc<Mutex<Channel>>, tonic::transport::Error> {
        if let Some(channel) = self.shared_channel.take() {
            Ok(channel)
        } else {
            self.shared_channel = Some(Arc::new(Mutex::new(self.create_channel()?)));
            Ok(self.shared_channel.clone().unwrap())
        }
    }

    pub fn create_channel(&self) -> Result<Channel, tonic::transport::Error> {
        let mut endpoint = Endpoint::from_shared(self.server_address.clone())
            .expect("Invalid server address")
            .connect_timeout(CONNECTION_TIMEOUT);
        if let Some(tls_config) = &self.tls_config {
            endpoint = endpoint.tls_config(tls_config.clone())?;
        }
        Ok(endpoint.connect_lazy())
    }
}

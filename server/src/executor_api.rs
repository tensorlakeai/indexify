#[allow(non_camel_case_types)] // The autogenerated code uses snake_case types in some cases
pub mod executor_api_pb {
    tonic::include_proto!("executor_api_pb");
}

use std::{collections::HashMap, pin::Pin, sync::Arc, time::Instant, vec};

use anyhow::Result;
use executor_api_pb::{
    executor_api_server::ExecutorApi,
    AllowedFunction,
    DataPayloadEncoding,
    DesiredExecutorState,
    ExecutorState,
    ExecutorStatus,
    FunctionExecutorResources,
    FunctionExecutorStatus,
    GetDesiredExecutorStatesRequest,
    HostResources,
    ReportExecutorStateRequest,
    ReportExecutorStateResponse,
    TaskAllocation,
    TaskResult,
};
use tokio::sync::watch::{self, Receiver, Sender};
use tokio_stream::{wrappers::WatchStream, Stream};
use tonic::{Request, Response, Status};
use tracing::{debug, error, info, instrument, trace, warn};

use crate::{
    blob_store::{self, BlobStorage},
    data_model::{
        self,
        Allocation,
        DataPayload,
        ExecutorId,
        ExecutorMetadata,
        ExecutorMetadataBuilder,
        FunctionAllowlist,
        FunctionExecutor,
        FunctionExecutorDiagnostics,
        FunctionExecutorId,
        GPUResources,
        GraphVersion,
        NodeOutputBuilder,
        TaskDiagnostics,
        TaskFailureReason,
        TaskOutcome,
    },
    executor_api::executor_api_pb::{FunctionExecutorState, FunctionExecutorTerminationReason},
    executors::ExecutorManager,
    metrics::api_io_stats,
    state_store::{
        requests::{
            AllocationOutput,
            RequestPayload,
            StateMachineUpdateRequest,
            UpsertExecutorRequest,
        },
        IndexifyState,
    },
};

impl TryFrom<AllowedFunction> for FunctionAllowlist {
    type Error = anyhow::Error;

    fn try_from(allowed_function: AllowedFunction) -> Result<Self, Self::Error> {
        let version = allowed_function.graph_version.map(|v| GraphVersion(v));
        Ok(FunctionAllowlist {
            namespace: allowed_function.namespace,
            compute_graph_name: allowed_function.graph_name,
            compute_fn_name: allowed_function.function_name,
            version,
        })
    }
}

impl TryFrom<data_model::HostResources> for HostResources {
    type Error = anyhow::Error;

    fn try_from(from: data_model::HostResources) -> Result<Self, Self::Error> {
        Ok(HostResources {
            // int division is okay because cpu_ms_per_sec derived from host hardware CPU cores is
            // always a multiple of 1000.
            cpu_count: Some(from.cpu_ms_per_sec / 1000),
            memory_bytes: Some(from.memory_bytes),
            disk_bytes: Some(from.disk_bytes),
            gpu: from.gpu.map(|g| g.try_into()).transpose()?,
        })
    }
}

impl TryFrom<HostResources> for data_model::HostResources {
    type Error = anyhow::Error;

    fn try_from(from: HostResources) -> Result<Self, Self::Error> {
        let cpu = from
            .cpu_count
            .ok_or(anyhow::anyhow!("cpu_count is required"))?;
        let memory = from
            .memory_bytes
            .ok_or(anyhow::anyhow!("memory_bytes is required"))?;
        let disk = from
            .disk_bytes
            .ok_or(anyhow::anyhow!("disk_bytes is required"))?;
        let gpu = from.gpu.map(|g| g.try_into()).transpose()?;
        Ok(data_model::HostResources {
            cpu_ms_per_sec: cpu * 1000,
            memory_bytes: memory,
            disk_bytes: disk,
            gpu,
        })
    }
}

impl TryFrom<String> for DataPayloadEncoding {
    type Error = anyhow::Error;

    fn try_from(value: String) -> Result<Self, Self::Error> {
        match value.as_str() {
            "application/json" => Ok(DataPayloadEncoding::Utf8Json),
            "application/octet-stream" => Ok(DataPayloadEncoding::BinaryPickle),
            "text/plain" => Ok(DataPayloadEncoding::Utf8Text),
            _ => Err(anyhow::anyhow!("unknown data payload encoding")),
        }
    }
}

impl From<ExecutorStatus> for data_model::ExecutorState {
    fn from(status: ExecutorStatus) -> Self {
        match status {
            ExecutorStatus::StartingUp => data_model::ExecutorState::StartingUp,
            ExecutorStatus::Running => data_model::ExecutorState::Running,
            ExecutorStatus::Drained => data_model::ExecutorState::Drained,
            ExecutorStatus::Stopped => data_model::ExecutorState::Stopped,
            ExecutorStatus::Unknown => data_model::ExecutorState::Unknown,
        }
    }
}

impl TryFrom<data_model::GPUResources> for executor_api_pb::GpuResources {
    type Error = anyhow::Error;

    fn try_from(gpu_resources: data_model::GPUResources) -> Result<Self, Self::Error> {
        if gpu_resources.count == 0 {
            return Err(anyhow::anyhow!("data_model gpu_resources.count is 0"));
        }
        let proto_model = match gpu_resources.model.as_str() {
            data_model::GPU_MODEL_NVIDIA_A100_40GB => Ok(executor_api_pb::GpuModel::NvidiaA10040gb),
            data_model::GPU_MODEL_NVIDIA_A100_80GB => Ok(executor_api_pb::GpuModel::NvidiaA10080gb),
            data_model::GPU_MODEL_NVIDIA_H100_80GB => Ok(executor_api_pb::GpuModel::NvidiaH10080gb),
            data_model::GPU_MODEL_NVIDIA_TESLA_T4 => Ok(executor_api_pb::GpuModel::NvidiaTeslaT4),
            data_model::GPU_MODEL_NVIDIA_A6000 => Ok(executor_api_pb::GpuModel::NvidiaA6000),
            data_model::GPU_MODEL_NVIDIA_A10 => Ok(executor_api_pb::GpuModel::NvidiaA10),
            _ => Err(anyhow::anyhow!("unknown data_model gpu_resources.model")),
        }?;
        Ok(executor_api_pb::GpuResources {
            count: Some(gpu_resources.count),
            model: Some(proto_model.into()),
        })
    }
}

impl TryFrom<executor_api_pb::GpuResources> for data_model::GPUResources {
    type Error = anyhow::Error;

    fn try_from(gpu_resources: executor_api_pb::GpuResources) -> Result<Self, Self::Error> {
        if gpu_resources.count() == 0 {
            return Err(anyhow::anyhow!("proto gpu_resources.count is 0"));
        }
        let str_model = match gpu_resources.model() {
            executor_api_pb::GpuModel::Unknown => {
                Err(anyhow::anyhow!("proto gpu_resources.model is unknown"))
            }
            executor_api_pb::GpuModel::NvidiaA10040gb => Ok(data_model::GPU_MODEL_NVIDIA_A100_40GB),
            executor_api_pb::GpuModel::NvidiaA10080gb => Ok(data_model::GPU_MODEL_NVIDIA_A100_80GB),
            executor_api_pb::GpuModel::NvidiaH10080gb => Ok(data_model::GPU_MODEL_NVIDIA_H100_80GB),
            executor_api_pb::GpuModel::NvidiaTeslaT4 => Ok(data_model::GPU_MODEL_NVIDIA_TESLA_T4),
            executor_api_pb::GpuModel::NvidiaA6000 => Ok(data_model::GPU_MODEL_NVIDIA_A6000),
            executor_api_pb::GpuModel::NvidiaA10 => Ok(data_model::GPU_MODEL_NVIDIA_A10),
        }?;
        Ok(data_model::GPUResources {
            count: gpu_resources.count(),
            model: str_model.into(),
        })
    }
}

impl From<data_model::FunctionRetryPolicy> for executor_api_pb::TaskRetryPolicy {
    fn from(from: data_model::FunctionRetryPolicy) -> Self {
        executor_api_pb::TaskRetryPolicy {
            max_retries: Some(from.max_retries),
            initial_delay_ms: Some(from.initial_delay_ms),
            delay_multiplier: Some(from.delay_multiplier),
            max_delay_ms: Some(from.max_delay_ms),
        }
    }
}

impl TryFrom<FunctionExecutorResources> for data_model::FunctionExecutorResources {
    type Error = anyhow::Error;

    fn try_from(from: FunctionExecutorResources) -> Result<Self, Self::Error> {
        let cpu_ms_per_sec = from
            .cpu_ms_per_sec
            .ok_or(anyhow::anyhow!("cpu_ms_per_sec is required"))?;
        let memory_bytes = from
            .memory_bytes
            .ok_or(anyhow::anyhow!("memory_bytes is required"))?;
        let ephemeral_disk_bytes = from
            .disk_bytes
            .ok_or(anyhow::anyhow!("disk_bytes is required"))?;
        Ok(data_model::FunctionExecutorResources {
            cpu_ms_per_sec,
            // int division is okay because all the values were initially in MB and GB.
            memory_mb: (memory_bytes / 1024 / 1024) as u64,
            ephemeral_disk_mb: (ephemeral_disk_bytes / 1024 / 1024) as u64,
            gpu: from.gpu.map(|g| GPUResources::try_from(g)).transpose()?,
        })
    }
}

impl TryFrom<data_model::FunctionExecutorResources> for FunctionExecutorResources {
    type Error = anyhow::Error;

    fn try_from(from: data_model::FunctionExecutorResources) -> Result<Self, Self::Error> {
        Ok(FunctionExecutorResources {
            cpu_ms_per_sec: Some(from.cpu_ms_per_sec),
            memory_bytes: Some(from.memory_mb as u64 * 1024 * 1024),
            disk_bytes: Some(from.ephemeral_disk_mb as u64 * 1024 * 1024),
            gpu: from
                .gpu
                .map(|g| {
                    g.try_into()
                        .map_err(|e| anyhow::anyhow!("failed to convert GPU resources: {}", e))
                })
                .transpose()?,
        })
    }
}

impl TryFrom<ExecutorState> for ExecutorMetadata {
    type Error = anyhow::Error;

    fn try_from(executor_state: ExecutorState) -> Result<Self, Self::Error> {
        let mut executor_metadata = ExecutorMetadataBuilder::default();
        let executor_id = executor_state
            .executor_id
            .clone()
            .map(ExecutorId::new)
            .ok_or(anyhow::anyhow!("executor_id is required"))?;
        executor_metadata.id(executor_id.clone());
        executor_metadata.state(executor_state.status().into());
        if let Some(state_hash) = executor_state.state_hash.clone() {
            executor_metadata.state_hash(state_hash);
        }
        if let Some(executor_version) = executor_state.version {
            executor_metadata.executor_version(executor_version);
        }
        let mut allowed_functions = Vec::new();
        for function in executor_state.allowed_functions {
            allowed_functions.push(FunctionAllowlist::try_from(function)?);
        }
        if allowed_functions.is_empty() {
            executor_metadata.function_allowlist(None);
        } else {
            executor_metadata.function_allowlist(Some(allowed_functions));
        }
        if let Some(addr) = executor_state.hostname {
            executor_metadata.addr(addr);
        }
        let mut labels = HashMap::new();
        for (key, value) in executor_state.labels {
            labels.insert(key, serde_json::Value::String(value));
        }
        executor_metadata.labels(labels);
        let mut function_executors = HashMap::new();
        for function_executor_state in executor_state.function_executor_states {
            let function_executor =
                data_model::FunctionExecutor::try_from(function_executor_state)?;
            function_executors.insert(function_executor.id.clone(), function_executor);
        }
        executor_metadata.function_executors(function_executors);
        if let Some(host_resources) = executor_state.total_function_executor_resources {
            let cpu = host_resources
                .cpu_count
                .ok_or(anyhow::anyhow!("cpu_count is required"))?;
            let memory = host_resources
                .memory_bytes
                .ok_or(anyhow::anyhow!("memory_bytes is required"))?;
            let disk = host_resources
                .disk_bytes
                .ok_or(anyhow::anyhow!("disk_bytes is required"))?;
            // Ignore errors during conversion as they are expected e.g. if Executor GPU
            // model is unknown.
            let gpu = match host_resources.gpu {
                Some(gpu_resources) => gpu_resources.try_into().ok(),
                None => None,
            };
            executor_metadata.host_resources(data_model::HostResources {
                cpu_ms_per_sec: cpu * 1000,
                memory_bytes: memory,
                disk_bytes: disk,
                gpu,
            });
            executor_metadata.host_resources(host_resources.try_into()?);
        }
        if let Some(server_clock) = executor_state.server_clock {
            executor_metadata.clock(server_clock);
        }
        Ok(executor_metadata.build()?)
    }
}

impl TryFrom<FunctionExecutorTerminationReason> for data_model::FunctionExecutorTerminationReason {
    type Error = anyhow::Error;

    fn try_from(
        termination_reason: FunctionExecutorTerminationReason,
    ) -> Result<Self, Self::Error> {
        match termination_reason {
            FunctionExecutorTerminationReason::Unknown => {
                Ok(data_model::FunctionExecutorTerminationReason::Unknown)
            }
            FunctionExecutorTerminationReason::StartupFailedInternalError => {
                Ok(data_model::FunctionExecutorTerminationReason::StartupFailedInternalError)
            }
            FunctionExecutorTerminationReason::StartupFailedFunctionError => {
                Ok(data_model::FunctionExecutorTerminationReason::StartupFailedFunctionError)
            }
            FunctionExecutorTerminationReason::StartupFailedFunctionTimeout => {
                Ok(data_model::FunctionExecutorTerminationReason::StartupFailedFunctionTimeout)
            }
            FunctionExecutorTerminationReason::Unhealthy => {
                Ok(data_model::FunctionExecutorTerminationReason::Unhealthy)
            }
            FunctionExecutorTerminationReason::InternalError => {
                Ok(data_model::FunctionExecutorTerminationReason::InternalError)
            }
            FunctionExecutorTerminationReason::FunctionTimeout => {
                Ok(data_model::FunctionExecutorTerminationReason::FunctionError)
            }
            FunctionExecutorTerminationReason::FunctionCancelled => {
                Ok(data_model::FunctionExecutorTerminationReason::FunctionCancelled)
            }
        }
    }
}

impl TryFrom<FunctionExecutorState> for data_model::FunctionExecutor {
    type Error = anyhow::Error;

    fn try_from(function_executor_state: FunctionExecutorState) -> Result<Self, Self::Error> {
        let termination_reason = data_model::FunctionExecutorTerminationReason::try_from(
            function_executor_state.termination_reason(),
        )?;
        let id = function_executor_state
            .description
            .as_ref()
            .map(|description| description.id.clone())
            .flatten()
            .ok_or(anyhow::anyhow!("id is required"))?;
        let namespace = function_executor_state
            .description
            .as_ref()
            .map(|description| description.namespace.clone())
            .flatten()
            .ok_or(anyhow::anyhow!("namespace is required"))?;
        let compute_graph_name = function_executor_state
            .description
            .as_ref()
            .map(|description| description.graph_name.clone())
            .flatten()
            .ok_or(anyhow::anyhow!("compute_graph_name is required"))?;
        let compute_fn_name = function_executor_state
            .description
            .as_ref()
            .map(|description| description.function_name.clone())
            .flatten()
            .ok_or(anyhow::anyhow!("compute_fn_name is required"))?;
        let version = function_executor_state
            .description
            .as_ref()
            .map(|description| description.graph_version.clone())
            .flatten()
            .ok_or(anyhow::anyhow!("version is required"))?;
        let resources = function_executor_state
            .description
            .as_ref()
            .map(|description| description.resources.clone())
            .flatten()
            .ok_or(anyhow::anyhow!("resources is required"))?;
        let resources = data_model::FunctionExecutorResources::try_from(resources.clone())?;
        Ok(FunctionExecutor {
            id: FunctionExecutorId::new(id.clone()),
            namespace: namespace.clone(),
            compute_graph_name: compute_graph_name.clone(),
            compute_fn_name: compute_fn_name.clone(),
            version: GraphVersion(version.clone()),
            state: match function_executor_state.status() {
                FunctionExecutorStatus::Unknown => data_model::FunctionExecutorState::Unknown,
                FunctionExecutorStatus::Pending => data_model::FunctionExecutorState::Pending,
                FunctionExecutorStatus::Running => data_model::FunctionExecutorState::Running,
                FunctionExecutorStatus::Terminated => {
                    data_model::FunctionExecutorState::Terminated {
                        reason: termination_reason,
                        failed_alloc_ids: function_executor_state.allocation_ids_caused_termination,
                    }
                }
            },
            resources,
        })
    }
}

fn to_function_executor_diagnostics(
    function_executor_update: &executor_api_pb::FunctionExecutorUpdate,
    blob_storage: &BlobStorage,
) -> Result<FunctionExecutorDiagnostics> {
    let description = function_executor_update
        .description
        .as_ref()
        .ok_or(anyhow::anyhow!("description is required"))?;
    let id = description
        .id
        .clone()
        .ok_or(anyhow::anyhow!("id is required"))?;
    let namespace = description
        .namespace
        .clone()
        .ok_or(anyhow::anyhow!("namespace is required"))?;
    let graph_name = description
        .graph_name
        .clone()
        .ok_or(anyhow::anyhow!("graph_name is required"))?;
    let function_name = description
        .function_name
        .clone()
        .ok_or(anyhow::anyhow!("function_name is required"))?;
    let graph_version = description
        .graph_version
        .clone()
        .ok_or(anyhow::anyhow!("graph_version is required"))?;
    let startup_stdout = prepare_data_payload(
        function_executor_update.startup_stdout.clone(),
        &blob_storage.get_url_scheme(),
        &blob_storage.get_url(),
    );
    let startup_stderr = prepare_data_payload(
        function_executor_update.startup_stderr.clone(),
        &blob_storage.get_url_scheme(),
        &blob_storage.get_url(),
    );

    Ok(data_model::FunctionExecutorDiagnostics {
        id: FunctionExecutorId::new(id.to_string()),
        namespace,
        graph_name,
        function_name,
        graph_version: GraphVersion(graph_version),
        startup_stdout,
        startup_stderr,
    })
}

fn to_function_executor_diagnostics_vector(
    function_executor_updates: &Vec<executor_api_pb::FunctionExecutorUpdate>,
    blob_storage: &BlobStorage,
) -> Result<Vec<FunctionExecutorDiagnostics>> {
    function_executor_updates
        .iter()
        .map(|function_executor_update| {
            to_function_executor_diagnostics(function_executor_update, blob_storage)
        })
        .collect()
}

pub struct ExecutorAPIService {
    indexify_state: Arc<IndexifyState>,
    executor_manager: Arc<ExecutorManager>,
    api_metrics: Arc<api_io_stats::Metrics>,
    blob_storage: Arc<blob_store::BlobStorage>,
}

impl ExecutorAPIService {
    pub fn new(
        indexify_state: Arc<IndexifyState>,
        executor_manager: Arc<ExecutorManager>,
        api_metrics: Arc<api_io_stats::Metrics>,
        blob_storage: Arc<blob_store::BlobStorage>,
    ) -> Self {
        Self {
            indexify_state,
            executor_manager,
            api_metrics,
            blob_storage,
        }
    }

    pub async fn handle_task_outcomes(
        &self,
        executor_id: ExecutorId,
        task_results: Vec<TaskResult>,
    ) -> Result<Vec<AllocationOutput>> {
        let mut allocation_output_updates = Vec::new();
        for task_result in task_results {
            self.api_metrics
                .fn_outputs
                .add(task_result.function_outputs.len() as u64, &[]);
            let task_id = task_result
                .task_id
                .clone()
                .ok_or(Status::invalid_argument("task_id is required"))?;
            let outcome_code =
                executor_api_pb::TaskOutcomeCode::try_from(task_result.outcome_code.unwrap_or(0))
                    .map_err(|e| Status::invalid_argument(e.to_string()))?;
            let failure_reason = task_result
                .failure_reason
                .map(|reason| {
                    executor_api_pb::TaskFailureReason::try_from(reason)
                        .map_err(|e| Status::invalid_argument(e.to_string()))
                })
                .transpose()?;
            let namespace = task_result
                .namespace
                .clone()
                .ok_or(Status::invalid_argument("namespace is required"))?;
            let compute_graph = task_result
                .graph_name
                .clone()
                .ok_or(Status::invalid_argument("compute_graph is required"))?;
            let compute_fn = task_result
                .function_name
                .clone()
                .ok_or(Status::invalid_argument("compute_fn is required"))?;
            let invocation_id = task_result
                .graph_invocation_id
                .clone()
                .ok_or(Status::invalid_argument("graph_invocation_id is required"))?;
            let allocation_id = task_result
                .allocation_id
                .clone()
                .ok_or(anyhow::anyhow!("allocation_id is required"))?;
            let Some(task) = self
                .indexify_state
                .reader()
                .get_task(
                    &namespace,
                    &compute_graph,
                    &invocation_id,
                    &compute_fn,
                    &task_id,
                )
                .map_err(|e| Status::internal(e.to_string()))?
            else {
                warn!("Task not found for task_id: {}", task_id);
                continue;
            };

            let Some(compute_graph) = self.indexify_state.reader().get_compute_graph_version(
                &namespace,
                &compute_graph,
                &task.graph_version,
            )?
            else {
                warn!("Compute graph version not found for task_id: {}", task_id);
                continue;
            };

            let Some(compute_fn_node) = compute_graph.nodes.get(&compute_fn) else {
                warn!("Compute fn node not found for task_id: {}", task_id);
                continue;
            };

            let mut payloads = Vec::new();
            let mut encoding_str = String::new();
            for output in task_result.function_outputs.clone() {
                let url = output
                    .uri
                    .ok_or(Status::invalid_argument("uri is required"))?;

                let path = blob_store_url_to_path(
                    &url,
                    &self.blob_storage.get_url_scheme(),
                    &self.blob_storage.get_url(),
                );
                let size = output
                    .size
                    .ok_or(Status::invalid_argument("size is required"))?;
                let sha256_hash = output
                    .sha256_hash
                    .ok_or(Status::invalid_argument("sha256_hash is required"))?;
                let data_payload = DataPayload {
                    path,
                    size,
                    sha256_hash,
                };
                encoding_str = match output.encoding {
                    Some(value) => {
                        let output_encoding = DataPayloadEncoding::try_from(value)
                            .map_err(|e| Status::invalid_argument(e.to_string()))?;
                        match output_encoding {
                            DataPayloadEncoding::Utf8Json => Ok("application/json"),
                            DataPayloadEncoding::BinaryPickle => Ok("application/octet-stream"),
                            DataPayloadEncoding::Utf8Text => Ok("text/plain"),
                            DataPayloadEncoding::BinaryZip => Ok("application/zip"),
                            DataPayloadEncoding::Unknown => {
                                Err(Status::invalid_argument("unknown data payload encoding"))
                            }
                        }
                    }
                    None => Err(Status::invalid_argument(
                        "data payload encoding is required",
                    )),
                }?
                .to_string();
                payloads.push(data_payload);
            }
            let invocation_error_payload = prepare_data_payload(
                task_result.invocation_error_output.clone(),
                &self.blob_storage.get_url_scheme(),
                &self.blob_storage.get_url(),
            );

            let node_output = NodeOutputBuilder::default()
                .namespace(namespace.to_string())
                .compute_graph_name(compute_graph.compute_graph_name.clone())
                .invocation_id(invocation_id.to_string())
                .compute_fn_name(compute_fn.to_string())
                .payloads(payloads)
                .next_functions(task_result.next_functions.clone())
                .encoding(encoding_str.to_string())
                .allocation_id(allocation_id.clone())
                .invocation_error_payload(invocation_error_payload)
                .reducer_output(compute_fn_node.reducer)
                .build()
                .map_err(|e| Status::internal(e.to_string()))?;

            let task_diagnostic = TaskDiagnostics {
                stdout: prepare_data_payload(
                    task_result.stdout.clone(),
                    &self.blob_storage.get_url_scheme(),
                    &self.blob_storage.get_url(),
                ),
                stderr: prepare_data_payload(
                    task_result.stderr.clone(),
                    &self.blob_storage.get_url_scheme(),
                    &self.blob_storage.get_url(),
                ),
            };
            let allocation_key = Allocation::key_from(
                namespace.as_str(),
                compute_graph.compute_graph_name.as_str(),
                invocation_id.as_str(),
                allocation_id.as_str(),
            );
            let mut allocation = self
                .indexify_state
                .reader()
                .get_allocation(&allocation_key)
                .map_err(|e| Status::internal(e.to_string()))?
                .ok_or(anyhow::anyhow!("allocation not found"))?;
            let allocation_failure_reason = match failure_reason {
                Some(reason) => match reason {
                    executor_api_pb::TaskFailureReason::Unknown => Some(TaskFailureReason::Unknown),
                    executor_api_pb::TaskFailureReason::InternalError => {
                        Some(TaskFailureReason::InternalError)
                    }
                    executor_api_pb::TaskFailureReason::FunctionError => {
                        Some(TaskFailureReason::FunctionError)
                    }
                    executor_api_pb::TaskFailureReason::FunctionTimeout => {
                        Some(TaskFailureReason::FunctionTimeout)
                    }
                    executor_api_pb::TaskFailureReason::InvocationError => {
                        Some(TaskFailureReason::InvocationError)
                    }
                    executor_api_pb::TaskFailureReason::TaskCancelled => {
                        Some(TaskFailureReason::TaskCancelled)
                    }
                    executor_api_pb::TaskFailureReason::FunctionExecutorTerminated => {
                        Some(TaskFailureReason::FunctionExecutorTerminated)
                    }
                },
                None => None,
            };
            let task_outcome = match outcome_code {
                executor_api_pb::TaskOutcomeCode::Success => TaskOutcome::Success,
                executor_api_pb::TaskOutcomeCode::Failure => {
                    let failure_reason =
                        allocation_failure_reason.unwrap_or(TaskFailureReason::Unknown);
                    TaskOutcome::Failure(failure_reason)
                }
                executor_api_pb::TaskOutcomeCode::Unknown => TaskOutcome::Unknown,
            };
            allocation.outcome = task_outcome;
            allocation.diagnostics = Some(task_diagnostic.clone());

            let request = AllocationOutput {
                namespace: namespace.to_string(),
                compute_graph: compute_graph.compute_graph_name.clone(),
                compute_fn: compute_fn.to_string(),
                invocation_id: invocation_id.to_string(),
                node_output,
                executor_id: executor_id.clone(),
                allocation,
                allocation_key,
            };
            allocation_output_updates.push(request);
        }
        Ok(allocation_output_updates)
    }
}

fn log_desired_executor_state_delta(
    last_sent_state: &DesiredExecutorState,
    desired_state: &DesiredExecutorState,
) {
    debug!(?desired_state, "got desired state");

    let mut last_assignments: HashMap<String, String> = HashMap::default();
    for ta in &last_sent_state.task_allocations {
        if let (Some(fe_id), Some(alloc_id)) = (&ta.function_executor_id, &ta.allocation_id) {
            last_assignments.insert(fe_id.clone(), alloc_id.clone());
        }
    }

    for TaskAllocation {
        function_executor_id: fn_executor_id_option,
        allocation_id: allocation_id_option,
        ..
    } in &desired_state.task_allocations
    {
        if let (Some(fn_executor_id), Some(allocation_id)) =
            (fn_executor_id_option, allocation_id_option)
        {
            match last_assignments.get(fn_executor_id) {
                Some(last_allocation_id) => {
                    if allocation_id != last_allocation_id {
                        info!(
                            %fn_executor_id,
                            %allocation_id, %last_allocation_id, "re-assigning FE"
                        )
                    }
                    last_assignments.remove(fn_executor_id);
                }
                None => {
                    info!(%fn_executor_id, %allocation_id, "assigning FE")
                }
            }
        }
    }

    for (fn_executor_id, last_allocation_id) in last_assignments {
        info!(%fn_executor_id, %last_allocation_id, "idling FE")
    }
}

#[instrument(skip_all, fields(executor_id = %executor_id))]
async fn executor_update_loop(
    executor_id: ExecutorId,
    executor_manager: Arc<ExecutorManager>,
    mut executor_state_rx: Receiver<()>,
    grpc_tx: Sender<Result<DesiredExecutorState, Status>>,
) {
    // Mark the state as changed to trigger the first change
    // notification to the executor. This is important because between
    // the report_executor_state and the get_desired_executor_states
    // requests, the executor state may received a new desired state.
    executor_state_rx.mark_changed();

    // Use the default/empty value for the last-sent desired state, so
    // that the first change logged will be a delta from nothing (=>
    // the complete executor state).
    let mut last_sent_state = DesiredExecutorState::default();

    loop {
        // Wait for a state change for this executor or grpc stream closing.
        tokio::select! {
            _ = grpc_tx.closed() => {
                info!(
                    "get_desired_executor_states: grpc stream closed"
                );
                break;
            }
            result = executor_state_rx.changed() => {
                if let Err(err) = result {
                    info!(
                        ?err,
                        "state machine watcher closing"
                    );
                    break;
                }
            }
        }

        // Get the latest state
        let desired_state = executor_manager.get_executor_state(&executor_id).await;

        // Log the state delta
        log_desired_executor_state_delta(&last_sent_state, &desired_state);

        // Send the state to the executor
        if let Err(err) = grpc_tx.send(Ok(desired_state.clone())) {
            info!(?err, "grpc stream closing");
            break;
        }

        // Store the sent state for next comparison
        last_sent_state = desired_state;
    }
}

#[tonic::async_trait]
impl ExecutorApi for ExecutorAPIService {
    #[allow(non_camel_case_types)] // The autogenerated code in the trait uses snake_case types in some cases
    type get_desired_executor_statesStream =
        Pin<Box<dyn Stream<Item = Result<DesiredExecutorState, Status>> + Send>>;

    async fn report_executor_state(
        &self,
        request: Request<ReportExecutorStateRequest>,
    ) -> Result<Response<ReportExecutorStateResponse>, Status> {
        let start = Instant::now();
        let executor_state = request
            .get_ref()
            .executor_state
            .clone()
            .ok_or(Status::invalid_argument("executor_state is required"))?;
        let executor_id = executor_state
            .executor_id
            .clone()
            .ok_or(Status::invalid_argument("executor_id is required"))?;
        let executor_id = ExecutorId::new(executor_id);
        let executor_update = request
            .get_ref()
            .executor_update
            .clone()
            .ok_or(Status::invalid_argument("executor_update is required"))?;

        trace!(
            executor_id = executor_id.get(),
            "Got report_executor_state request"
        );

        let function_executor_diagnostics = to_function_executor_diagnostics_vector(
            &executor_update.function_executor_updates,
            &self.blob_storage,
        )
        .map_err(|e| Status::invalid_argument(e.to_string()))?;

        let executor_metadata = ExecutorMetadata::try_from(executor_state)
            .map_err(|e| Status::invalid_argument(e.to_string()))?;
        let executor_state_updated = self
            .executor_manager
            .heartbeat(&executor_metadata)
            .await
            .map_err(|e| Status::internal(e.to_string()))?;

        let task_results = executor_update.task_results.clone();
        let allocation_output_updates = self
            .handle_task_outcomes(executor_id.clone(), task_results)
            .await
            .map_err(|e| Status::internal(e.to_string()))?;

        let sm_req = StateMachineUpdateRequest {
            payload: RequestPayload::UpsertExecutor(UpsertExecutorRequest {
                executor: executor_metadata,
                function_executor_diagnostics,
                executor_state_updated,
                allocation_outputs: allocation_output_updates,
            }),
            processed_state_changes: vec![],
        };
        if let Err(e) = self.indexify_state.write(sm_req).await {
            error!(
                executor_id = executor_id.get(),
                "failed to write state machine update request: {:?}", e
            );
            return Err(Status::internal(
                "failed to write state machine update request",
            ));
        }

        let duration_sec = start.elapsed().as_secs_f64();
        if duration_sec >= 1.0 {
            warn!(
                executor_id = executor_id.get(),
                duration_sec = duration_sec,
                "report_executor_state took too long"
            );
        }

        Ok(Response::new(ReportExecutorStateResponse {}))
    }

    async fn get_desired_executor_states(
        &self,
        request: Request<GetDesiredExecutorStatesRequest>,
    ) -> Result<Response<Self::get_desired_executor_statesStream>, Status> {
        let executor_id = request
            .get_ref()
            .executor_id
            .clone()
            .ok_or(Status::invalid_argument("executor_id is required"))?;
        let executor_id = ExecutorId::new(executor_id);

        trace!(
            executor_id = executor_id.get(),
            "Got get_desired_executor_states request",
        );

        let executor_state_rx = match self.executor_manager.subscribe(&executor_id).await {
            Some(state_rx) => state_rx,
            None => {
                let msg = "executor not found, or not yet registered";
                warn!(
                    executor_id = executor_id.get(),
                    "get_desired_executor_states: {}", msg
                );
                return Err(Status::not_found(msg));
            }
        };

        let (grpc_tx, grpc_rx) = watch::channel(Result::Ok(DesiredExecutorState {
            function_executors: vec![],
            task_allocations: vec![],
            clock: Some(0),
        }));
        tokio::spawn(executor_update_loop(
            executor_id,
            self.executor_manager.clone(),
            executor_state_rx,
            grpc_tx,
        ));

        let grpc_stream = WatchStream::from_changes(grpc_rx);
        Ok(Response::new(
            Box::pin(grpc_stream) as Self::get_desired_executor_statesStream
        ))
    }
}

fn prepare_data_payload(
    msg: Option<executor_api_pb::DataPayload>,
    blob_store_url_scheme: &str,
    blob_store_url: &str,
) -> Option<data_model::DataPayload> {
    if msg.is_none() {
        return None;
    }
    let msg = msg.unwrap();
    if msg.uri.is_none() {
        return None;
    }
    if msg.size.as_ref().is_none() {
        return None;
    }
    if msg.sha256_hash.as_ref().is_none() {
        return None;
    }

    Some(data_model::DataPayload {
        path: blob_store_url_to_path(&msg.uri.unwrap(), blob_store_url_scheme, blob_store_url),
        size: msg.size.unwrap(),
        sha256_hash: msg.sha256_hash.unwrap(),
    })
}

pub fn blob_store_path_to_url(
    path: &str,
    blob_store_url_scheme: &str,
    blob_store_url: &str,
) -> String {
    if blob_store_url_scheme == "file" {
        // Local file blob store implementation is always using absolute paths without
        // "/"" prefix. The paths are not relative to the configure blob_store_url path.
        return format!("{}:///{}", blob_store_url_scheme, path);
    } else if blob_store_url_scheme == "s3" {
        // S3 blob store implementation uses paths relative to its bucket from
        // blob_store_url.
        return format!(
            "{}://{}/{}",
            blob_store_url_scheme,
            bucket_name_from_s3_blob_store_url(blob_store_url),
            path
        );
    } else {
        return format!("not supported blob store scheme: {}", blob_store_url_scheme);
    }
}

pub fn blob_store_url_to_path(
    url: &str,
    blob_store_url_scheme: &str,
    blob_store_url: &str,
) -> String {
    if blob_store_url_scheme == "file" {
        // Local file blob store implementation is always using absolute paths without
        // "/"" prefix. The paths are not relative to the configure blob_store_url path.
        return url
            .strip_prefix(&format!("{}:///", blob_store_url_scheme).to_string())
            // The url doesn't include blob_store_scheme if this payload was uploaded to server
            // instead of directly to blob storage.
            .unwrap_or(url)
            .to_string();
    } else if blob_store_url_scheme == "s3" {
        // S3 blob store implementation uses paths relative to its bucket from
        // blob_store_url.
        return url
            .strip_prefix(
                &format!(
                    "{}://{}/",
                    blob_store_url_scheme,
                    bucket_name_from_s3_blob_store_url(blob_store_url)
                )
                .to_string(),
            )
            // The url doesn't include blob_store_url if this payload was uploaded to server instead
            // of directly to blob storage.
            .unwrap_or(url)
            .to_string();
    } else {
        return format!("not supported blob store scheme: {}", blob_store_url_scheme);
    }
}

fn bucket_name_from_s3_blob_store_url(blob_store_url: &str) -> String {
    match url::Url::parse(blob_store_url) {
        Ok(url) => match url.host_str() {
            Some(bucket) => bucket.into(),
            None => {
                error!("Didn't find bucket name in S3 url: {}", blob_store_url);
                return String::new();
            }
        },
        Err(e) => {
            error!(
                "Failed to parse blob_store_url: {}. Error: {}",
                blob_store_url, e
            );
            return String::new();
        }
    }
}

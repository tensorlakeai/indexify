import unittest
from indexify_extractors.sentence_transformer import SentenceTransformersEmbedding
from indexify_extractors.dpr import DPREmbeddings


class TestSTEmbeddings(unittest.TestCase):

    def __init__(self, *args, **kwargs):
        super(TestSTEmbeddings, self).__init__(*args, **kwargs)


    def test_query_embedding(self):
        st = SentenceTransformersEmbedding("all-MiniLM-L6-v2")
        embeddings = st.embed_query("hello world")
        self.assertEqual(len(embeddings), 384)

    def test_ctx_embeddings(self):
        st = SentenceTransformersEmbedding("all-MiniLM-L6-v2")
        embeddings = st.embed_ctx(["hello", "world"])
        self.assertEqual(len(embeddings), 2)
        self.assertEqual(len(embeddings[0]), 384)

    def test_tokenize(self):
        st = SentenceTransformersEmbedding("all-MiniLM-L6-v2")
        chunks = st.tokenize(["hello", "world hi"])
        self.assertEqual(len(chunks), 2)
        #self.assertEqual(len(chunks[0]), 1)
        self.assertEqual(chunks[1], "world hi")

    def test_token_decode_encode(self):
        st = SentenceTransformersEmbedding("all-MiniLM-L6-v2")
        tokens = st.tokenizer_encode(["hello", "world hi"])
        texts = st.tokenizer_decode(tokens)
        self.assertEqual(len(texts), 2)
        self.assertAlmostEqual(texts[0], "hello")
        self.assertAlmostEqual(texts[1], "world hi")
 


class TestDPR(unittest.TestCase):

    def __init__(self, *args, **kwargs):
        super(TestDPR, self).__init__(*args, **kwargs)

    def test_query_embedding(self):
        st = DPREmbeddings("facebook-dpr-ctx_encoder-multiset-base", "facebook-dpr-question_encoder-multiset-base")
        embeddings = st.embed_query("hello world")
        self.assertEqual(len(embeddings), 768)

    def test_ctx_embeddings(self):
        st = DPREmbeddings("facebook-dpr-ctx_encoder-multiset-base", "facebook-dpr-question_encoder-multiset-base")
        embeddings = st.embed_ctx(["hello", "world"])
        self.assertEqual(len(embeddings), 2)
        self.assertEqual(len(embeddings[0]), 768)

    def test_token_decode_encode(self):
        st = DPREmbeddings("facebook-dpr-ctx_encoder-multiset-base", "facebook-dpr-question_encoder-multiset-base")
        tokens = st.tokenizer_encode(["hello", "world hi"])
        texts = st.tokenizer_decode(tokens)
        self.assertEqual(len(texts), 2)
        self.assertAlmostEqual(texts[0], "hello")
        self.assertAlmostEqual(texts[1], "world hi")

    def test_tokenize(self):
        st = DPREmbeddings("facebook-dpr-ctx_encoder-multiset-base", "facebook-dpr-question_encoder-multiset-base")
        chunks = st.tokenize(["hello", "world hi"])
        self.assertEqual(len(chunks), 2)
        #self.assertEqual(len(chunks[0]), 1)
        self.assertEqual(chunks[1], "world hi")

if __name__ == "__main__":
    unittest.main()
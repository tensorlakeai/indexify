---
title: Introduction
description: Effortless Ingestion and Extraction for Unstructured Data at Any Scale
---

<p align="center">
  ![Indexify High Level](/images/docs_intro_diagram.png)
</p>


Indexify is an open-source data framework. It is designed for creating ingestion and extraction pipelines for unstructured data. The framework uses a declarative approach to perform structured extraction with any AI model or to transform ingested data.

Indexify pipelines operate in real-time. They process data immediately after ingestion. This makes them ideal for interactive applications and low-latency scenarios.

**Use Cases:** Pipelines for Entity Extraction/Embedding from documents, Audio Transcription, Summarization, Object Detection from Images. 

**RAG**: Embedding and structured data extracted by Indexify from enterprise data can be consumed into RAG applications.  

## Multi-Stage Data Ingestion and Extraction Workflows

Extraction Graphs are at the center of Indexify. They are pipelines which processes data and writes the output to databases for retrieval. Extraction policies are linked using the `content_source` attribute. Some unique characteristics of Indexify workflows are -

**AI Native**: Use models from OpenAI, HuggingFace and Ollama in the pipeline.

**Extensible**: by plugging in Python modules in the pipeline.

**Real Time and Scalable**: Handles real-time processing of data at scale powered by a low latency and distributed scheduler.

**APIs**: Pipelines are exposed as HTTP APIs, making them accessible from applications written in TypeScript, Java, Python, Go, etc.

```yaml graph.yaml
name: 'pdf-ingestion-pipeline'
extraction_policies:
- extractor: 'tensorlake/marker'
  name: 'pdf_to_markdown'
- extractor: 'tensorlake/ner'
  name: 'entity_extractor'
  content_source: 'pdf_to_markdown'
- extractor: 'tensorlake/minilm-l6'
  name: 'embedding'
  content_source: 'pdf_to_markdown'
```

<CodeGroup>
    ```bash cURL
    curl -X 'POST' \
    'http://localhost:8900/namespaces/default/extraction_graphs' \
    -H 'Content-Type: application/x-yaml' \
    --data-binary @graph.yaml
    ```
    ```python Python
    from indexify import IndexifyClient, ExtractionGraph
    graph = ExtractionGraph.from_yaml_file("graph.yaml")
    client = IndexifyClient()
    client.create_extraction_graph(graph)
    ```
    ```typescript Typescript
    import { IndexifyClient, ExtractionGraph } from 'getindexify';
    import * as fs from 'fs';
    import * as yaml from 'js-yaml';

    async function main() {
      // Load the YAML file
      const graphYaml = fs.readFileSync('graph.yaml', 'utf8');
      const graphData = yaml.load(graphYaml) as Record<string, any>;

      // Create ExtractionGraph from the YAML data
      const graph = new ExtractionGraph(graphData.name, graphData.extraction_policies);

      // Create IndexifyClient
      const client = await IndexifyClient.createClient();

      // Create extraction graph
      await client.createExtractionGraph(graph);

      console.log('Extraction graph created successfully');
    }

    main().catch(console.error);
    ```
</CodeGroup>

### Continuous Data Ingestion

<CodeGroup>
    ```bash cURL
    curl -X 'POST' \
    'http://localhost:8900/namespaces/default/extraction_graphs/pdf-ingestion-pipeline/extract' \
    -H 'Content-Type: multipart/form-data' \
    -F 'file=@file.pdf;type=application/pdf' \
    -F 'labels={"source":"arxiv"}'
    ```
    ```python
    client = IndexifyClient()

    files = ["file1.pdf", .... "file100.pdf"]
    for file in files:
    client.upload_file("pdf-ingestion-pipeline", file)
    ```
    ```typescript
    import { IndexifyClient } from 'getindexify';
    import * as fs from 'fs';
    import * as path from 'path';

    async function uploadFiles() {
      const client = await IndexifyClient.createClient();

      const files = Array.from({ length: 100 }, (_, i) => `file${i + 1}.pdf`);

      for (const file of files) {
        try {
          const filePath = path.join(__dirname, file);
          const fileContent = fs.readFileSync(filePath);
          const fileBlob = new Blob([fileContent], { type: 'application/pdf' });

          const contentId = await client.uploadFile('pdf-ingestion-pipeline', fileBlob, { filename: file });
          console.log(`Successfully uploaded ${file}. Content ID: ${contentId}`);
        } catch (error) {
          console.error(`Error uploading ${file}:`, error);
        }
      }
    }

    uploadFiles().catch(console.error);
    ```
</CodeGroup>

### Retrieve Extracted Data

Retrieve extracted named entities 

<CodeGroup>
    ```bash cURL
    curl -X 'GET' \
    'http://localhost:8900/namespaces/:namespace/extraction_graphs/pdf-ingestion-pipeline/content/585ebafdbc9dbbbe/extraction_policies/entity_extractor' \
    ```
    ```python Python
    content_ids = [content.id for content in client.list_content("pdf-ingestion-pipeline")]

    markdown = client.get_extracted_content(content_ids[0], "pdf-ingestion-pipeline", "pdf_to_markdown")
    named_entities = client.get_extracted_content(content_ids[0], "pdf-ingestion-pipeline", "entity_extractor")
    ```
    ```typescript TypeScript
    import { IndexifyClient, IContentMetadata } from 'getindexify';

    async function listAndExtractContent() {
      const client = await IndexifyClient.createClient();

      // List content and extract content IDs
      const { contentList } = await client.listContent('pdf-ingestion-pipeline');
      const contentIds = contentList.map(content => content.id);

      if (contentIds.length === 0) {
        console.log('No content found in the pdf-ingestion-pipeline');
        return;
      }

      // Get extracted content for the first content ID
      const firstContentId = contentIds[0];

      try {
        const { contentList: markdownContent } = await client.getExtractedContent({
          contentId: firstContentId,
          graphName: 'pdf-ingestion-pipeline',
          policyName: 'pdf_to_markdown'
        });

        console.log('Markdown content:', markdownContent);

        const { contentList: namedEntities } = await client.getExtractedContent({
          contentId: firstContentId,
          graphName: 'pdf-ingestion-pipeline',
          policyName: 'entity_extractor'
        });

        console.log('Named entities:', namedEntities);
      } catch (error) {
        console.error('Error getting extracted content:', error);
      }
    }

    listAndExtractContent().catch(console.error);
    ```
</CodeGroup>

Search vector indexes populated by embeddings

<CodeGroup>
    ```bash cURL
    curl -X 'POST' \
    'http://localhost:8900/namespaces/default/indexes/pdf-ingestion-pipeline.embedding.embedding/search' \
    -H 'Content-Type: application/json' \
    -d '{
        "filters": [],
        "include_content": true,
        "k": 3,
        "query": "wework"
    }'
    ```
    ```python
    results = client.search("pdf-ingestion-pipeline.embedding.embedding","Who won the 2017 NBA finals?", k=3)
    ```
    ```typescript
    import { IndexifyClient, ISearchIndexResponse } from 'getindexify';

    async function performSemanticSearch() {
      const client = await IndexifyClient.createClient();

      try {
        const results: ISearchIndexResponse[] = await client.searchIndex(
          'pdf-ingestion-pipeline.embedding.embedding',
          'Who won the 2017 NBA finals?',
          3  // This is the 'k' parameter in the Python version
        );

        console.log('Search results:', results);

        // Process and display the results
        results.forEach((result, index) => {
          console.log(`Result ${index + 1}:`);
          console.log(`Content:`, result.content);
          console.log(`Score:`, result.score);
          console.log(`Metadata:`, result.metadata);
          console.log('---');
        });
      } catch (error) {
        console.error('Error performing semantic search:', error);
      }
    }

    performSemanticSearch().catch(console.error);
    ```
</CodeGroup>

To understand usage of indexify for various use cases we recommend the following starting points.

<Card title="Basic Tutorial" icon="link" href="docs/getting-started-basic">
  A Wikipedia ingestion and indexing pipeline. The tutorial teaches the basics of Indexify.
  It introduces all the system components and how to build a simple pipeline.
</Card>
<Card title="Intermediate Tutorial" icon="link" href="docs/getting-started-intermediate">
  A tax document processing and Q&A system. The tutorial covers text extraction from PDF and building a RAG pipeline.
</Card>
<Card title="Multi-Modal RAG" icon="link" href="https://github.com/tensorlakeai/indexify/tree/main/examples/pdf/indexing_and_rag">
  A detailed example of text, table and image extraction from PDF. It also covers building image and text indexes and doing 
  cross-modal retrieval, re-ranking, and reciproal rank fusion.
</Card>

## Why Use Indexify?

Indexify is for continuous ingestion and extraction from unstructured data in a streaming fashion. If you are building LLM Application that need structured data from any unstructured data sources, Indexify is the tool for you.

While there lies a divide in tools for prototyping and for deploying to production, Indexify runs locally on laptops for rapid prototyping, but you can also deploy a scaled up version in production for high availability and reliability.

You should use Indexify if you care about -

* Processing Multi-Modal Data such as PDFs, Videos, Images, and Audio
* High Availability and Fault Tolerance
* Local Experience for rapid prototyping
* Automatically updating indexes whenever upstream data sources are updated
* Incremental Extraction and Selective Deletion
* Compatibility with Various LLM Frameworks (Langchain, DSPy, etc.)
* Integration with any database, blob store or vector store.
* Owning your data infrastructure and being able to deploy on any cloud.

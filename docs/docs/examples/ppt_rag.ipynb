{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Introduction**\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://getindexify.ai/\"><img src=\"https://getindexify.ai/Indexify_Logo_Wordmark.svg\" width=\"145\"></a>\n",
        "  <a href=\"https://discord.com/invite/kF8UZACA7r\"><img src=\"https://raw.githubusercontent.com/rishiraj/random/main/Discord%20button.png\" width=\"145\"></a><br>\n",
        "  Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/tensorlakeai/indexify\">Github</a></i> ⭐\n",
        "</div>\n",
        "\n",
        "This notebook demonstrates how Indexify can make it easier to quickly extract insights from complex real-world PowerPoint presentations like a talk given on \"[A little guide to building Large Language Models in 2024](https://docs.google.com/presentation/d/1IkzESdOwdmwvPxIELYJi8--K3EZ98_cL6c5ZcLKSyVg/edit?usp=sharing)\" by Thomas, the co-founder of Hugging Face. Using the slides as an example, we show how the Indexify library can enable question answering on the talk to get rapid answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9NVuXFG3qGw"
      },
      "source": [
        "## **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install indexify indexify-extractor-sdk\n",
        "\n",
        "# Download Indexify Server\n",
        "!curl https://getindexify.ai | sh\n",
        "\n",
        "# Download Extractors\n",
        "!indexify-extractor download hub://text/chunking\n",
        "!indexify-extractor download hub://embedding/minilm-l6\n",
        "!indexify-extractor download hub://pdf/presentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEHbUyCM3-vm"
      },
      "source": [
        "After installing the necessary libraries, download the server, and the extractors, you need to restart the runtime. Then, you have to run Indexify Server with the Extractors.\n",
        "\n",
        "Open 2 terminals and run the following commands:\n",
        "\n",
        "```bash\n",
        "# Terminal 1\n",
        "./indexify server -d\n",
        "\n",
        "# Terminal 2\n",
        "indexify-extractor join-server\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Test the extractors**\n",
        "\n",
        "We will try PPTExtractor first. The PPTExtractor can extract all the values from text as well as tables in one shot and passes it to the next chained extractors which can be used for question answering.\n",
        "\n",
        "We'll start by downloading the talk's slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "req = requests.get(\"https://raw.githubusercontent.com/tensorlakeai/indexify/main/docs/docs/files/test.pptx\")\n",
        "\n",
        "with open('test.pptx','wb') as f:\n",
        "    f.write(req.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from indexify_extractor_sdk import load_extractor, Content\n",
        "\n",
        "pptextractor, pptconfig_cls = load_extractor(\"presentations.ppt_extractor:PPTExtractor\")\n",
        "content = Content.from_file(\"test.pptx\")\n",
        "config = pptconfig_cls()\n",
        "\n",
        "ppt_result = pptextractor.extract(content, config)\n",
        "text_content = next(content.data.decode('utf-8') for content in ppt_result if content.content_type == 'text/plain')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A little guide to building Large Language Models\u000bin 2024\n"
          ]
        }
      ],
      "source": [
        "print(text_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTIlKuPp6wxg"
      },
      "source": [
        "## **Create a Client**\n",
        "Instantiate the Indexify Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HZNysNl-631k"
      },
      "outputs": [],
      "source": [
        "from indexify import IndexifyClient\n",
        "client = IndexifyClient()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Question Answering Task**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQr1749x6_CW"
      },
      "source": [
        "### **Extraction Graph Setup**\n",
        "\n",
        "1. Import the `ExtractionGraph` class from the `indexify` package.\n",
        "\n",
        "2. Define the extraction graph specification in YAML format:\n",
        "   - Set the name of the extraction graph to \"pptqa\".\n",
        "   - Define the extraction policies:\n",
        "     - Use the \"tensorlake/ppt\" extractor for PPT marking and name it \"docextractor\".\n",
        "     - Use the \"tensorlake/chunk-extractor\" for text chunking and name it \"chunks\".\n",
        "       - Set the input parameters for the chunker:\n",
        "         - `chunk_size`: 1000 (size of each text chunk)\n",
        "         - `overlap`: 100 (overlap between chunks)\n",
        "         - `content_source`: \"docextractor\" (source of content for chunking)\n",
        "     - Use the \"tensorlake/arctic\" extractor for embedding and name it \"get-embeddings\".\n",
        "       - Set the content source for embedding to \"chunks\".\n",
        "\n",
        "3. Create an `ExtractionGraph` object from the YAML specification using `ExtractionGraph.from_yaml()`.\n",
        "\n",
        "4. Create the extraction graph on the Indexify client using `client.create_extraction_graph()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from indexify import ExtractionGraph\n",
        "\n",
        "extraction_graph_spec = \"\"\"\n",
        "name: 'pptqa'\n",
        "extraction_policies:\n",
        "   - extractor: 'tensorlake/ppt'\n",
        "     name: 'docextractor'\n",
        "   - extractor: 'tensorlake/chunk-extractor'\n",
        "     name: 'chunker'\n",
        "     input_params:\n",
        "        chunk_size: 1000\n",
        "        overlap: 100\n",
        "     content_source: 'docextractor'\n",
        "   - extractor: 'tensorlake/arctic'\n",
        "     name: 'embedder'\n",
        "     content_source: 'chunker'\n",
        "\"\"\"\n",
        "\n",
        "extraction_graph = ExtractionGraph.from_yaml(extraction_graph_spec)\n",
        "client.create_extraction_graph(extraction_graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGqpkx3P7gsh"
      },
      "source": [
        "### **Upload the talk's PPT slides**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETideBqK8GGp"
      },
      "outputs": [],
      "source": [
        "content_id = client.upload_file(\"pptqa\", \"test.pptx\")\n",
        "client.wait_for_extraction(content_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2WDexIU8LFy"
      },
      "source": [
        "### **What is happening behind the scenes**\n",
        "\n",
        "Indexify is designed to seamlessly respond to ingestion events by assessing all existing policies and triggering the necessary extractors for extraction. Once the PPT extractor completes the process of extracting texts, bytes, and JSONs from the document, it automatically initiates the embedding extractor to chunk the content, extract embeddings, and populate an index.\n",
        "\n",
        "With Indexify, you have the ability to upload hundreds of PPT files simultaneously, and the platform will efficiently handle the extraction and indexing of the contents without requiring manual intervention. To expedite the extraction process, you can deploy multiple instances of the extractors, and Indexify's built-in scheduler will transparently distribute the workload among them, ensuring optimal performance and efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6SQ0xDt9a_9"
      },
      "source": [
        "### **Perform RAG with OpenAI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2raD6aeB9Th1"
      },
      "outputs": [],
      "source": [
        "def get_context(question: str, index: str, top_k=3):\n",
        "    results = client.search_index(name=index, query=question, top_k=top_k)\n",
        "    context = \"\"\n",
        "    for result in results:\n",
        "        context = context + f\"content id: {result['content_id']} \\n\\n passage: {result['text']}\\n\"\n",
        "    return context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'content id: 629da2991e51ccc0 \\n\\n passage: Focus on efficient pretraining while taking a holistic view of model life-cycle\\ncontent id: bdfec28ce8239696 \\n\\n passage: Start by test existing models on your domain and task(s) of interest\\ncontent id: c4fc86ea0bf467a4 \\n\\n passage: When the model is too big:\\nTensor Parallelism\\n'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"What are the things to keep in mind to finetune a pretrained model?\"\n",
        "context = get_context(question, \"pptqa.embedder.embedding\")\n",
        "context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_prompt(question, context):\n",
        "    return f\"Answer the question, based on the context.\\n question: {question} \\n context: {context}\"\n",
        "\n",
        "prompt = create_prompt(question, context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client_openai = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckexWnEe-B3c"
      },
      "source": [
        "Now ask any question related to the ingested talk ppt slides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cSc4uBLA-IEB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the context provided, to finetune a pretrained model, one should focus on efficient pretraining and take a holistic view of the model's life cycle. It is also important to start by testing existing models on your specific domain and tasks of interest. Additionally, when the model is too big, one strategy is to use Tensor Parallelism.\n"
          ]
        }
      ],
      "source": [
        "chat_completion = client_openai.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        ")\n",
        "print(chat_completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

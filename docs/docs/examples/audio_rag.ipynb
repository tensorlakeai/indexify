{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c82beb96",
   "metadata": {},
   "source": [
    "# **RAG using Audio as a Context**\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://getindexify.ai/\"><img src=\"https://getindexify.ai/Indexify_Logo_Wordmark.svg\" width=\"145\"></a>\n",
    "  <a href=\"https://discord.com/invite/kF8UZACA7r\"><img src=\"https://raw.githubusercontent.com/rishiraj/random/main/Discord%20button.png\" width=\"145\"></a><br>\n",
    "  Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/tensorlakeai/indexify\">Github</a></i> ⭐\n",
    "</div>\n",
    "\n",
    "This notebook will show you how to use audio files as a context for your RAG pipeline. We are going to use 2 Indexify Extractors:\n",
    "\n",
    "- `tensorlake/whisper-asr`: This extractor will convert the audio file into text.\n",
    "- `tensorlake/minilm-l6`: This extractor will convert the text into embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54f1d9",
   "metadata": {},
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ed358a-f1ed-4527-8d43-31b6c5cd65fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install accelerate ffmpeg indexify\n",
    "\n",
    "# Download Indexify Server\n",
    "!curl https://getindexify.ai | sh\n",
    "\n",
    "# Download Extractors\n",
    "!indexify-extractor download hub://audio/whisper-asr\n",
    "!indexify-extractor download hub://embedding/minilm-l6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cdde57",
   "metadata": {},
   "source": [
    "After installing the necessary libraries, download the server, and the extractors, you need to restart the runtime. Then, you have to run Indexify Server with the Extractors.\n",
    "\n",
    "Open 2 terminals and run the following commands:\n",
    "\n",
    "```bash\n",
    "# Terminal 1\n",
    "./indexify server -d\n",
    "\n",
    "# Terminal 2\n",
    "indexify-extractor join-server\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046d776",
   "metadata": {},
   "source": [
    "## **Create Extraction Graph**\n",
    "\n",
    "To create a great extraction graph, we need to understand the input data type that we are working with and the output data type that we want to achieve. In this case, we are working with audio files and we want to get relevant text by their embeddings.\n",
    "\n",
    "For that, we are going to create 2 extraction policies:\n",
    "\n",
    "1. Audio to Text\n",
    "2. Text to Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12c018d1-b4e6-4fb1-a33a-76e3f6746992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexify import IndexifyClient\n",
    "client = IndexifyClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c63da",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_graph_spec = \"\"\"\n",
    "name: \"audio-knowledgebase\"\n",
    "extraction_policies:\n",
    "   - extractor: \"tensorlake/whisper-asr\"\n",
    "     name: \"transcription\"\n",
    "\n",
    "   - extractor: \"tensorlake/minilm-l6\"\n",
    "     name: \"transcription-embedding\"\n",
    "     content_source: \"transcription_chunks\"\n",
    "\"\"\"\n",
    "\n",
    "extraction_graph = ExtractionGraph.from_yaml(extraction_graph_spec)\n",
    "client.create_extraction_graph(extraction_graph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1241fd1a-700a-4862-b05c-66aad94bb6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the path to the audio file you want to upload.\n",
    "PATH = \"\"\n",
    "content_id = client.upload_file(\"audio-knowledgebase\", path=PATH)\n",
    "client.wait_for_extraction(content_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917df79",
   "metadata": {},
   "source": [
    "## **Indexify Retriever for RAG**\n",
    "\n",
    "After the process is completed, we can use IndexifyRetriever to retrieve the most relevant documents for a given query using the index created by the MiniLM Extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7822afff-b9f0-4a79-82c2-29aeead2427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexify_langchain import IndexifyRetriever\n",
    "params = {\"name\": \"audio-knowledgebase.transcription-embedding.embedding\", \"top_k\": 50}\n",
    "retriever = IndexifyRetriever(client=client, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7145884-60c8-4ae3-ad43-550ee344a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74736cf9-1ea0-46df-957f-486a85b7a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI(openai_api_key=\"xxx\")\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0834b5e",
   "metadata": {},
   "source": [
    "## **Ask Questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1da434f0-1a5f-476c-ab2d-dd4c726e37cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Grok is a company that has had a significant viral moment in its history recently. It was founded in 2016 and has been a long road for the company. The company has seen a surge in customers and interest, with 3,000 unique customers trying to consume their resources in a short period, ranging from Fortune 500 companies to developers. The company has been fortunate to experience this growth and potential disruption in the market. Time will tell how big the company can get, but there is a lot of market cap for Grok to gain by producing things at scale. The company has been described as a meager unicorn, with a last valuation of around a billion dollars. The potential for Grok to be disruptive in the market is significant, and it has had a very exciting and important moment in its history recently.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Tell me about Grok\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

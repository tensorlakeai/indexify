{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG on Multiple Terms and Conditions Documents Varying By Geography\n",
    "In this demo we are going to build a pipeline to build and update policy documents which vary by geography. \n",
    "Approach \n",
    "* Label documents during ingestion\n",
    "* Propogate the labels on the documents all the way into the vector store\n",
    "* During Retrieval make the LLM generate filters with labels based on the question \n",
    "* Pass the label filters into the vector store for retrieval \n",
    "* Make the LLM cite the sources of the response during response synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9NVuXFG3qGw"
   },
   "source": [
    "### Install the Indexify Extractor SDK, Langchain Retriever and the Indexify Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udB1A9ee1RFv"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install indexify-extractor-sdk indexify openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGrirc_G3zSI"
   },
   "source": [
    "### Start the Indexify Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prTC7y7i3UCu"
   },
   "outputs": [],
   "source": [
    "!./indexify server -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEHbUyCM3-vm"
   },
   "source": [
    "### Download an Embedding Extractor\n",
    "On another terminal we'll download and start the embedding extractor which we will use to index text from the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYQmZBoR4UsY"
   },
   "outputs": [],
   "source": [
    "!indexify-extractor download hub://embedding/minilm-l6\n",
    "!indexify-extractor join-server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a Chunk Extractor\n",
    "The documents will have to be chunked so that the paragraphs are not longer than what the embedding model can support "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!indexify-extractor download hub://text/chunking\n",
    "!indexify-extractor join-server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfA8GSfG4cF7"
   },
   "source": [
    "### Download the PDF Extractor\n",
    "On another terminal we'll install the necessary dependencies and start the PDF extractor which we will use to get text, bytes or json out of PDF documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBUpNXkV5vZ_"
   },
   "source": [
    "Install Poppler on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lk6YqXfG5_De"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install -y poppler-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3EDV6Xk6LbU"
   },
   "source": [
    "Download and start the PDF extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_167kZaE6b3Q"
   },
   "outputs": [],
   "source": [
    "!indexify-extractor download hub://pdf/pdf-extractor\n",
    "!indexify-extractor join-server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTIlKuPp6wxg"
   },
   "source": [
    "### Create Extraction Policies\n",
    "Instantiate the Indexify Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZNysNl-631k"
   },
   "outputs": [],
   "source": [
    "from indexify import IndexifyClient, ExtractionGraph\n",
    "client = IndexifyClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQr1749x6_CW"
   },
   "source": [
    "First, create an Extraction Graph with policies to get texts and contents out of the PDF and create chunks from the text and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uff8cBlq7Mrv"
   },
   "outputs": [],
   "source": [
    "extraction_graph_spec = \"\"\"\n",
    "name: 'knowledgebase'\n",
    "extraction_policies:\n",
    "  - extractor: 'tensorlake/pdf-extractor'\n",
    "    name: 'pdfextractor'\n",
    "  - extractor: 'tensorlake/chunk-extractor'\n",
    "    name: 'chunks'\n",
    "    content_source: 'pdfextractor'\n",
    "    input_params:\n",
    "      chunk_size: 512\n",
    "      overlap: 150\n",
    "  - extractor: 'tensorlake/minilm-l6'\n",
    "    name: 'terms'\n",
    "    content_source: 'chunks'\n",
    "\"\"\"\n",
    "extraction_graph = ExtractionGraph.from_yaml(extraction_graph_spec)\n",
    "client.create_extraction_graph(extraction_graph)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGqpkx3P7gsh"
   },
   "source": [
    "### Upload a PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eaw5wDEL79dy"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "req = requests.get(\"https://www.sixt.com/shared/t-c/sixt_US_en_CALIFORNIA.pdf\")\n",
    "req1 = requests.get(\"https://www.sixt.com/shared/t-c/sixt_US_en_HAWAII.pdf\")\n",
    "req2 = requests.get(\"https://www.sixt.com/shared/t-c/sixt_US_en_ILLINOIS.pdf\")\n",
    "\n",
    "\n",
    "with open('sixt_US_en_CALIFORNIA.pdf','wb') as f:\n",
    "    f.write(req.content)\n",
    "\n",
    "with open('sixt_US_en_HAWAII.pdf','wb') as f:\n",
    "    f.write(req.content)\n",
    "\n",
    "with open('sixt_US_en_ILLINOIS.pdf', 'wb') as f:\n",
    "    f.write(req.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETideBqK8GGp"
   },
   "outputs": [],
   "source": [
    "client.upload_file(path=\"sixt_US_en_CALIFORNIA.pdf\", labels={\"state\": \"california\"})\n",
    "client.upload_file(path=\"sixt_US_en_HAWAII.pdf\", labels={\"state\": \"hawaii\"})\n",
    "client.upload_file(path=\"sixt_US_en_ILLINOIS.pdf\", labels={\"state\": \"illinois\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2WDexIU8LFy"
   },
   "source": [
    "### What is happening behind the scenes\n",
    "\n",
    "Indexify is designed to seamlessly respond to ingestion events by assessing all existing policies and triggering the necessary extractors for extraction. Once the PDF extractor completes the process of extracting texts, bytes, and JSONs from the document, it automatically initiates the embedding extractor to chunk the content, extract embeddings, and populate an index.\n",
    "\n",
    "With Indexify, you have the ability to upload hundreds of PDF files simultaneously, and the platform will efficiently handle the extraction and indexing of the contents without requiring manual intervention. To expedite the extraction process, you can deploy multiple instances of the extractors, and Indexify's built-in scheduler will transparently distribute the workload among them, ensuring optimal performance and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6SQ0xDt9a_9"
   },
   "source": [
    "### Perform RAG\n",
    "Initialize the Langchain Retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "oai_client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=\"\",\n",
    ")\n",
    "\n",
    "def answer_question(question) -> str:\n",
    "    chat_completion = oai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"given the question {question}, if there is the name of a US state, generate a predicate such as state=texas or state=new york. The predicate name and value should be in small letters.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    query_filter = chat_completion.choices[0].message.content\n",
    "    query_filter\n",
    "    search_results = client.search_index(\"knowledgebase.terms.embedding\", question, top_k=5, filters=[query_filter])\n",
    "    context = \"\"\n",
    "    for result in search_results:\n",
    "        context += f\"content_id: {result['content_id']}\\n text: {result['text']}\\n\"\n",
    "    context\n",
    "    chat_completion = oai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\" Answer the question based on the context provided below and provide citation in the response as 'Citation: '. The context has the citation to content_ids and the text below it. \\n Context: {context} \\n \\n Question: {question}\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    print(chat_completion.choices[0].message.content)\n",
    "    chat_completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(\"If I rent a car from Sixt in California, how many days do I have to return the vehicle before being considered overdue??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(\"If I rent a car from Sixt in Hawaii, how many days do I have to return the vehicle before being considered overdue??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring OpenAI's multimodal capabilities with Indexify extractor\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://getindexify.ai/\"><img src=\"https://getindexify.ai/images/logos/base2.svg\" width=\"145\"></a>\n",
    "  <a href=\"https://discord.com/invite/kF8UZACA7r\"><img src=\"https://raw.githubusercontent.com/rishiraj/random/main/Discord%20button.png\" width=\"145\"></a><br>\n",
    "  Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/tensorlakeai/indexify\">Github</a></i> ⭐\n",
    "</div>\n",
    "\n",
    "## Creating a Extraction Pipeline is Simple with Indexify\n",
    "\n",
    "#### Install Indexify, Start the Server & Download the Extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install indexify indexify-extractor-sdk\n",
    "\n",
    "# Download Indexify Server\n",
    "!curl https://getindexify.ai | sh\n",
    "\n",
    "# Download Extractors\n",
    "!indexify-extractor download tensorlake/openai\n",
    "!indexify-extractor download tensorlake/chunk-extractor\n",
    "!indexify-extractor download tensorlake/arctic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing the necessary libraries, download the server, and the extractors, you need to restart the runtime. Then, you have to run Indexify Server with the Extractors.\n",
    "\n",
    "Open 2 terminals and run the following commands:\n",
    "\n",
    "```bash\n",
    "# Terminal 1\n",
    "./indexify server -d\n",
    "\n",
    "# Terminal 2\n",
    "indexify-extractor join-server\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Client, Define Extraction Graph & Ingest Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexify import IndexifyClient\n",
    "client = IndexifyClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Data Extraction from **Texts** with OpenAI\n",
    "\n",
    "The first example with Indexify's pipeline is to extract data, such as programming languages, from various textual sources using OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexify import ExtractionGraph\n",
    "\n",
    "extraction_graph_spec = \"\"\"\n",
    "name: 'resume_text'\n",
    "extraction_policies:\n",
    "   - extractor: 'tensorlake/openai'\n",
    "     name: 'pdfprocessor'\n",
    "     input_params:\n",
    "        model_name: 'gpt-3.5-turbo'\n",
    "        key: 'OPENAI_API_KEY'\n",
    "        prompt: 'Extract names of all programming languages from the text.'\n",
    "\"\"\"\n",
    "\n",
    "extraction_graph = ExtractionGraph.from_yaml(extraction_graph_spec)\n",
    "client.create_extraction_graph(extraction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"**First Last** \\nAdm. No. 22JEXXXX  \\nfirstlast@gmail.com   \\nXXX-XXX-XXXX\\nlinkedin.com/in/firstlast\\ngithub.com/firstlast \\n\\n**Education**\\n**University Name**\\nBachelor of Science in Computer Science (GPA: 4.00 / 4.00)\\n* **Relevant Coursework:** Data Structures and Algorithms (C++), Prob & Stat in CS (Python), Intro to CS II (C++), Linear Algebra w/Computational Applications (Python)\\n\\nExpected | May 20XX\\n------- | --------\\nCity, State\\n\\n**Experience**\\n\\n**Company Name 1** \\nJan 20XX - May 20XX\\nSoftware Engineer\\nCity, State\\n* Implemented microservices architecture using Node.js and Express, improving API response time by 25% and reducing server load by 30%.\\n* Led a cross-functional team in implementing a new feature using React and Redux, resulting in a 20% increase in user engagement within the first month.\\n* Optimized MySQL database queries, reducing page load times by 15% and enhancing overall application performance.\\n\\n**Projects**\\n\\n**Project Name 1** | React.js, Angular, Vue.js, Django, Flask, Ruby on Rails\\n* Led the development of a microservices-based e-commerce platform using Node.js, resulting in a 40% increase in daily transactions within the first quarter.\\n* Designed and deployed a scalable RESTful API using Django and Django REST Framework, achieving a 30% improvement in data retrieval speed.\\n* Implemented a real-time chat feature using WebSocket and Socket.io, enhancing user engagement and reducing response time by 20%.\\n\\n**Project Name 2** | Spring Boot, Express.js, TensorFlow, PyTorch, jQuery, Bootstrap\\n* Developed a data visualization dashboard using D3.js, providing stakeholders with real-time insights and improving decision-making processes.\\n* Built a CI/CD pipeline using Jenkins and Docker, reducing deployment time by 40% and ensuring consistent and reliable releases.\\n\\n**Technical Skills**\\n**Languages:** Rust, Kotlin, Swift, Go, Scala, TypeScript, R, Perl, Haskell, Groovy, Julia, Dart\\n**Technologies:** React.js, Angular, Vue.js, Django, Flask, Ruby on Rails, Spring Boot, Express.js, TensorFlow, PyTorch, jQuery, Bootstrap, Laravel, Flask, ASP.NET, Node.js, Electron, Android SDK, iOS SDK, Symfony\\n**Concepts:** Compiler, Operating System, Virtual Memory, Cache Memory, Encryption, Decryption, Artificial Intelligence, Machine Learning, Neural Networks, API, Database Normalization, Agile Methodology, Cloud Computing\\n\\n**Achievements**\\n* Pls Add your Achievements here e.g., Hackathons, Exam Ranks, etc. \\n\\n**Social Engagements**\\n**Vice-President:** Of Association of Exploration Geophysicist - Student Chapter, IIT Dhanbad\\n**Club Member:** at CYBER LABS -tech society of IIT Dhanbad\\n**Volunteer:** at KARTAVYA - NGO run by students of IIT Dhanbad to educate underprivileged childrens.\\n**Organiser:** Concetto'22 (Tech-fest) Khanan'22 (Geo-Mining fest).\\n**Sports-Engagements:** Badminton(state-level), chess, cricket, table-tennis. \\n\"\n",
    "content_id = client.add_documents(\"resume_text\", text)\n",
    "client.wait_for_extraction(content_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '4cf0d70a9a64dbc7',\n",
       "  'content': b'Here are the programming languages from the text, formatted as \"First Last\":\\n\\n* **C++**\\n* **Python**\\n* **Node.js** \\n* **React**\\n* **Redux**\\n* **MySQL**\\n* **React.js**\\n* **Angular**\\n* **Vue.js**\\n* **Django**\\n* **Flask**\\n* **Ruby on Rails**\\n* **Spring Boot**\\n* **Express.js**\\n* **TensorFlow**\\n* **PyTorch**\\n* **jQuery**\\n* **Bootstrap**\\n* **D3.js**\\n* **Jenkins**\\n* **Docker**\\n* **Rust**\\n* **Kotlin**\\n* **Swift**\\n* **Go**\\n* **Scala**\\n* **TypeScript**\\n* **R**\\n* **Perl**\\n* **Haskell**\\n* **Groovy**\\n* **Julia**\\n* **Dart**\\n* **Laravel**\\n* **ASP.NET** \\n* **Electron**\\n* **Android SDK**\\n* **iOS SDK**\\n* **Symfony** \\n'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_extracted_content(content_id, 'resume_text', 'pdfprocessor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Data Extraction from **Images** with OpenAI\n",
    "\n",
    "The second example with Indexify's pipeline is to extract data, such as programming languages, from various visual sources like images using OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexify import ExtractionGraph\n",
    "\n",
    "extraction_graph_spec = \"\"\"\n",
    "name: 'resume_images'\n",
    "extraction_policies:\n",
    "   - extractor: 'tensorlake/openai'\n",
    "     name: 'pdfprocessor'\n",
    "     input_params:\n",
    "        model_name: 'gpt-4o'\n",
    "        prompt: 'Extract names of all programming languages from the image.'\n",
    "\"\"\"\n",
    "\n",
    "extraction_graph = ExtractionGraph.from_yaml(extraction_graph_spec)\n",
    "client.create_extraction_graph(extraction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_id = client.upload_file(\"resume_images\", \"resume.jpg\")\n",
    "client.wait_for_extraction(content_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'f5faaadd623b8632',\n",
       "  'content': b'The programming languages mentioned in the image are:\\n\\n* **React.js**\\n* **Angular.js**\\n* **Vue.js**\\n* **Django**\\n* **Flask**\\n* **Ruby on Rails**\\n* **Node.js**\\n* **Express.js**\\n* **TensorFlow**\\n* **PyTorch**\\n* **jQuery**\\n* **Bootstrap**\\n* **D3.js**\\n* **Rust**\\n* **Kotlin**\\n* **Swift**\\n* **Go**\\n* **Scala**\\n* **TypeScript**\\n* **R**\\n* **Perl**\\n* **Haskell**\\n* **Groovy**\\n* **Julia**\\n* **Dart** \\n'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_extracted_content(content_id, 'resume_images', 'pdfprocessor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an end-to-end RAG pipeline with **PDF**\n",
    "#### Step 1: Direct Data Extraction from PDF with OpenAI\n",
    "\n",
    "The first step in Indexify's pipeline is to extract data, such as text, from various sources like PDF files. We understand that unstructured data poses a significant challenge and regular OCR based solutions can't always produce coherent & complete content. Hence, we use OpenAI's multimodal capabilities to do the extraction.\n",
    "\n",
    "#### Step 2: Enhanced Chunking with RecursiveCharacterTextSplitter\n",
    "\n",
    "Indexify's pipeline proceeds to perform chunking using the RecursiveCharacterTextSplitter algorithm. This algorithm has been specifically designed to handle large texts and create meaningful chunks based on a specified maximum chunk size.\n",
    "\n",
    "#### Step 3: Embedding Creation with Snowflake's Arctic Model\n",
    "\n",
    "The final step in Indexify's pipeline is the creation of embeddings using Snowflake's Arctic embedding model. Embeddings are critical for enabling efficient similarity search and retrieval of relevant information from the chunked text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexify import ExtractionGraph\n",
    "\n",
    "extraction_graph_spec = \"\"\"\n",
    "name: 'resumerag'\n",
    "extraction_policies:\n",
    "   - extractor: 'tensorlake/openai'\n",
    "     name: 'pdfprocessor'\n",
    "     input_params:\n",
    "        model_name: 'gpt-4o'\n",
    "        prompt: 'Extract all text from the document.'\n",
    "   - extractor: 'tensorlake/chunk-extractor'\n",
    "     name: 'chunker'\n",
    "     input_params:\n",
    "        chunk_size: 1000\n",
    "        overlap: 100\n",
    "     content_source: 'pdfprocessor'\n",
    "   - extractor: 'tensorlake/arctic'\n",
    "     name: 'embedder'\n",
    "     content_source: 'chunker'\n",
    "\"\"\"\n",
    "\n",
    "extraction_graph = ExtractionGraph.from_yaml(extraction_graph_spec)\n",
    "client.create_extraction_graph(extraction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "req = requests.get(\"https://www.overleaf.com/latex/templates/iit-dhanbad-resume-oncampus/sdtkcgtgxhtg.pdf\")\n",
    "\n",
    "with open('resume.pdf','wb') as f:\n",
    "    f.write(req.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_id = client.upload_file(\"resumerag\", \"resume.pdf\")\n",
    "client.wait_for_extraction(content_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '41c9b925a44596c3',\n",
       "  'content': b\"**First Last** \\nAdm. No. 22JEXXXX  \\nfirstlast@gmail.com   \\nXXX-XXX-XXXX\\nlinkedin.com/in/firstlast\\ngithub.com/firstlast \\n\\n**Education**\\n**University Name**\\nBachelor of Science in Computer Science (GPA: 4.00 / 4.00)\\n* **Relevant Coursework:** Data Structures and Algorithms (C++), Prob & Stat in CS (Python), Intro to CS II (C++), Linear Algebra w/Computational Applications (Python)\\n\\nExpected | May 20XX\\n------- | --------\\nCity, State\\n\\n**Experience**\\n\\n**Company Name 1** \\nJan 20XX - May 20XX\\nSoftware Engineer\\nCity, State\\n* Implemented microservices architecture using Node.js and Express, improving API response time by 25% and reducing server load by 30%.\\n* Led a cross-functional team in implementing a new feature using React and Redux, resulting in a 20% increase in user engagement within the first month.\\n* Optimized MySQL database queries, reducing page load times by 15% and enhancing overall application performance.\\n\\n**Projects**\\n\\n**Project Name 1** | React.js, Angular, Vue.js, Django, Flask, Ruby on Rails\\n* Led the development of a microservices-based e-commerce platform using Node.js, resulting in a 40% increase in daily transactions within the first quarter.\\n* Designed and deployed a scalable RESTful API using Django and Django REST Framework, achieving a 30% improvement in data retrieval speed.\\n* Implemented a real-time chat feature using WebSocket and Socket.io, enhancing user engagement and reducing response time by 20%.\\n\\n**Project Name 2** | Spring Boot, Express.js, TensorFlow, PyTorch, jQuery, Bootstrap\\n* Developed a data visualization dashboard using D3.js, providing stakeholders with real-time insights and improving decision-making processes.\\n* Built a CI/CD pipeline using Jenkins and Docker, reducing deployment time by 40% and ensuring consistent and reliable releases.\\n\\n**Technical Skills**\\n**Languages:** Rust, Kotlin, Swift, Go, Scala, TypeScript, R, Perl, Haskell, Groovy, Julia, Dart\\n**Technologies:** React.js, Angular, Vue.js, Django, Flask, Ruby on Rails, Spring Boot, Express.js, TensorFlow, PyTorch, jQuery, Bootstrap, Laravel, Flask, ASP.NET, Node.js, Electron, Android SDK, iOS SDK, Symfony\\n**Concepts:** Compiler, Operating System, Virtual Memory, Cache Memory, Encryption, Decryption, Artificial Intelligence, Machine Learning, Neural Networks, API, Database Normalization, Agile Methodology, Cloud Computing\\n\\n**Achievements**\\n* Pls Add your Achievements here e.g., Hackathons, Exam Ranks, etc. \\n\\n**Social Engagements**\\n**Vice-President:** Of Association of Exploration Geophysicist - Student Chapter, IIT Dhanbad\\n**Club Member:** at CYBER LABS -tech society of IIT Dhanbad\\n**Volunteer:** at KARTAVYA - NGO run by students of IIT Dhanbad to educate underprivileged childrens.\\n**Organiser:** Concetto'22 (Tech-fest) Khanan'22 (Geo-Mining fest).\\n**Sports-Engagements:** Badminton(state-level), chess, cricket, table-tennis. \\n\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_extracted_content(content_id, 'resumerag', 'pdfprocessor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing RAG with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(question: str, index: str, top_k=2):\n",
    "    results = client.search_index(name=index, query=question, top_k=top_k)\n",
    "    context = \"\"\n",
    "    for result in results:\n",
    "        context = context + f\"content id: {result['content_id']} \\n\\n passage: {result['text']}\\n\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the javascript related projects he has done?\"\n",
    "context = get_context(question, \"resumerag.embedder.embedding\")\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(question, context):\n",
    "    return f\"Answer the question, based on the context.\\n question: {question} \\n context: {context}\"\n",
    "\n",
    "prompt = create_prompt(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client_openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client_openai.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorlake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Installation and Setup**\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://getindexify.ai/\"><img src=\"https://getindexify.ai/Indexify_Logo_Wordmark.svg\" width=\"145\"></a>\n",
    "  <a href=\"https://discord.com/invite/kF8UZACA7r\"><img src=\"https://raw.githubusercontent.com/rishiraj/random/main/Discord%20button.png\" width=\"145\"></a><br>\n",
    "  Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/tensorlakeai/indexify\">Github</a></i> ⭐\n",
    "</div>\n",
    "\n",
    "1. Install the `indexify-extractor-sdk` package using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q indexify-extractor-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Download the required extractors:\n",
    "   - `hub://embedding/minilm-l6`: An embedding extractor based on the MiniLM-L6 model.\n",
    "   - `hub://text/chunking`: A text chunking extractor.\n",
    "   - `hub://pdf/marker`: A PDF marker extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!indexify-extractor download hub://embedding/minilm-l6\n",
    "!indexify-extractor download hub://text/chunking\n",
    "!indexify-extractor download hub://pdf/marker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Start the Indexify Extractor server on a separate terminal using the `indexify-extractor join-server` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!indexify-extractor join-server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Install the `indexify` package using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q indexify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Indexify Client Setup**\n",
    "\n",
    "1. Import the `IndexifyClient` class from the `indexify` package.\n",
    "2. Create an instance of the `IndexifyClient` called `client`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexify import IndexifyClient\n",
    "client = IndexifyClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create an Extraction Graph**\n",
    "\n",
    "1. Import the `ExtractionGraph` class fr\n",
    "2. Define the extraction graph specification in YAML format:\n",
    "   - Set the name of the extraction graph to \"pdfqa\".\n",
    "   - Define the extraction policies:\n",
    "     - Use the \"tensorlake/marker\" extractor for PDF marking and name it \"mdextract\".\n",
    "     - Use the \"tensorlake/chunk-extractor\" for text chunking and name it \"chunker\".\n",
    "       - Set the input parameters for the chunker:\n",
    "         - `chunk_size`: 1000 (size of each text chunk)\n",
    "         - `overlap`: 100 (overlap between chunks)\n",
    "         - `content_source`: \"mdextract\" (source of content for chunking)\n",
    "     - Use the \"tensorlake/minilm-l6\" extractor for embedding and name it \"pdfembedding\".\n",
    "       - Set the content source for embedding to \"chunker\".\n",
    "3. Create an `ExtractionGraph` object from the YAML specification using `ExtractionGraph.from_yaml()`.\n",
    "4. Create the extraction graph on the Indexify client using `client.create_extraction_graph()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexify import ExtractionGraph\n",
    "\n",
    "extraction_graph_spec = \"\"\"\n",
    "name: 'pdfqa'\n",
    "extraction_policies:\n",
    "   - extractor: 'tensorlake/marker'\n",
    "     name: 'mdextract'\n",
    "   - extractor: 'tensorlake/chunk-extractor'\n",
    "     name: 'chunker'\n",
    "     input_params:\n",
    "        chunk_size: 1000\n",
    "        overlap: 100\n",
    "     content_source: 'mdextract'\n",
    "   - extractor: 'tensorlake/minilm-l6'\n",
    "     name: 'pdfembedding'\n",
    "     content_source: 'chunker'\n",
    "\"\"\"\n",
    "\n",
    "extraction_graph = ExtractionGraph.from_yaml(extraction_graph_spec)\n",
    "client.create_extraction_graph(extraction_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Document Ingestion**\n",
    "\n",
    "1. Add the PDF document to the \"pdfqa\" extraction graph using `client.upload_file()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_id = client.upload_file(\"pdfqa\", \"chess.pdf\")\n",
    "client.wait_for_extraction(content_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Context Retrieval Function**\n",
    "\n",
    "1. Define a function called `get_context` that takes a question, index name, and top_k as parameters.\n",
    "\n",
    "2. Search the specified index using `client.search_index()` with the given question and top_k.\n",
    "\n",
    "3. Concatenate the retrieved passages into a single context string.\n",
    "\n",
    "4. Return the context string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(question: str, index: str, top_k=3):\n",
    "    results = client.search_index(name=index, query=question, top_k=3)\n",
    "    context = \"\"\n",
    "    for result in results:\n",
    "        context = context + f\"content id: {result['content_id']} \\n\\n passage: {result['text']}\\n\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prompt Creation Function**\n",
    "\n",
    "1. Define a function called `create_prompt` that takes a question and context as parameters.\n",
    "\n",
    "2. Create a prompt string that includes the question and context.\n",
    "\n",
    "3. Return the prompt string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(question, context):\n",
    "    return f\"Answer the question, based on the context.\\n question: {question} \\n context: {context}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question Answering**\n",
    "\n",
    "1. Define a question string.\n",
    "2. Call the `get_context` function with the question, index name (\"pdfqa.pdfembedding.embedding\"), and top_k (default is 3) to retrieve the relevant context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is the greatest player of all time and what is his record?\"\n",
    "context = get_context(question, \"pdfqa.pdfembedding.embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup OpenAI Client**\n",
    "\n",
    "1. Import the `OpenAI` class from the `openai` package.\n",
    "2. Create an instance of the `OpenAI` client called `client_openai` with the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client_openai = OpenAI(api_key=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Answering Question with OpenAI**\n",
    "\n",
    "1. Call the `create_prompt` function with the question and retrieved context to generate the prompt.\n",
    "2. Use the `client_openai.chat.completions.create()` method to send the prompt to the OpenAI API.\n",
    "   - Set the model to \"gpt-3.5-turbo\".\n",
    "   - Pass the prompt as a message with the \"user\" role.\n",
    "3. Print the generated answer from the API response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = create_prompt(question, context)\n",
    "\n",
    "chat_completion = client_openai.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorlake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

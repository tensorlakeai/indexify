{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9NVuXFG3qGw"
   },
   "source": [
    "### Install the Indexify Extractor SDK, Langchain Retriever and the Indexify Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udB1A9ee1RFv"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install indexify-extractor-sdk indexify-langchain indexify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGrirc_G3zSI"
   },
   "source": [
    "### Start the Indexify Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prTC7y7i3UCu"
   },
   "outputs": [],
   "source": [
    "!./indexify server -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEHbUyCM3-vm"
   },
   "source": [
    "### Download an Embedding Extractor\n",
    "On another terminal we'll download and start the embedding extractor which we will use to index text from the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYQmZBoR4UsY"
   },
   "outputs": [],
   "source": [
    "!indexify-extractor download hub://embedding/minilm-l6\n",
    "!indexify-extractor join-server minilm-l6.minilm_l6:MiniLML6Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download an Chunking Extractor\n",
    "On another terminal we'll download and start the chunking extractor that will create chunks from the text and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!indexify-extractor download hub://text/chunking\n",
    "!indexify-extractor join-server chunking.chunk_extractor:ChunkExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfA8GSfG4cF7"
   },
   "source": [
    "### Download the PDF Extractor\n",
    "On another terminal we'll install the necessary dependencies and start the PDF extractor which we will use to get text, bytes or json out of PDF documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBUpNXkV5vZ_"
   },
   "source": [
    "Install Poppler on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lk6YqXfG5_De"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install -y poppler-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3EDV6Xk6LbU"
   },
   "source": [
    "Download and start the PDF extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_167kZaE6b3Q"
   },
   "outputs": [],
   "source": [
    "!indexify-extractor download hub://pdf/pdf-extractor\n",
    "!indexify-extractor join-server pdf-extractor.pdf_extractor:PDFExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTIlKuPp6wxg"
   },
   "source": [
    "### Create Extraction Policies\n",
    "Instantiate the Indexify Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HZNysNl-631k"
   },
   "outputs": [],
   "source": [
    "from indexify import IndexifyClient\n",
    "client = IndexifyClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQr1749x6_CW"
   },
   "source": [
    "First, create a policy to get texts and contents out of the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "uff8cBlq7Mrv"
   },
   "outputs": [],
   "source": [
    "client.add_extraction_policy(extractor='tensorlake/pdf-extractor', name=\"pdf-extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JunwusCA7ZA5"
   },
   "source": [
    "Lastly, create chunks from the text and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.add_extraction_policy(extractor='tensorlake/chunk-extractor', name=\"chunks\", content_source=\"pdf-extraction\", input_params={\"chunk_size\": 512, \"overlap\": 150})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.add_extraction_policy(extractor='BAAI/bge-base-en', name=\"bge-embedding\", content_source=\"chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGqpkx3P7gsh"
   },
   "source": [
    "### Upload a PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Eaw5wDEL79dy"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "req = requests.get(\"https://arxiv.org/pdf/2310.16944.pdf\")\n",
    "\n",
    "with open('2310.16944.pdf','wb') as f:\n",
    "    f.write(req.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ETideBqK8GGp"
   },
   "outputs": [],
   "source": [
    "client.upload_file(path=\"2310.16944.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2WDexIU8LFy"
   },
   "source": [
    "### What is happening behind the scenes\n",
    "\n",
    "Indexify is designed to seamlessly respond to ingestion events by assessing all existing policies and triggering the necessary extractors for extraction. Once the PDF extractor completes the process of extracting texts, bytes, and JSONs from the document, it automatically initiates the embedding extractor to chunk the content, extract embeddings, and populate an index.\n",
    "\n",
    "With Indexify, you have the ability to upload hundreds of PDF files simultaneously, and the platform will efficiently handle the extraction and indexing of the contents without requiring manual intervention. To expedite the extraction process, you can deploy multiple instances of the extractors, and Indexify's built-in scheduler will transparently distribute the workload among them, ensuring optimal performance and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6SQ0xDt9a_9"
   },
   "source": [
    "### Perform RAG\n",
    "Initialize the Langchain Retreiver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "2raD6aeB9Th1"
   },
   "outputs": [],
   "source": [
    "from indexify_langchain import IndexifyRetriever\n",
    "params = {\"name\": \"bge-embedding.embedding\", \"top_k\": 15}\n",
    "retriever = IndexifyRetriever(client=client, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8Q1ulDM9u-e"
   },
   "source": [
    "Now create a chain to prompt OpenAI with data retreived from Indexify to create a simple Q&A bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "8FUO8cLA9unF"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "yfgv3imm9ZPG"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckexWnEe-B3c"
   },
   "source": [
    "Now ask any question related to the ingested PDF document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "cSc4uBLA-IEB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The results on MT-Bench show that Zephyr 7B model achieved a score of 7.34 and a win percentage of 90.60.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What are the results on MT-Bench?\")\n",
    "# Results on MT-Bench show that ZEPHYR-7B surpasses LLAMA 2-CHAT-70B, the best open-access RLHF-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The prompts being used are:\\n1. Prompt (turn 1): Imagine you are participating in a race with a group of people. If you have just overtaken the second person, what’s your current position? Where is the person you just overtook?\\n2. Prompt (turn 2): If the “second person” is changed to “last person” in the above question, what would the answer be?'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"what prompts are they using? Show all the prompts \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
